# ğŸ“Š Data Squad ê°€ì´ë“œ

## íŒ€ ì†Œê°œ

> **Data Squad**: RiskRadarì˜ ë°ì´í„° ìˆ˜ì§‘ ë° ìŠ¤íŠ¸ë¦¬ë° íŒŒì´í”„ë¼ì¸ ì „ë¬¸íŒ€
> 
> ì‹¤ì‹œê°„ ë‰´ìŠ¤ ë°ì´í„° ìˆ˜ì§‘ë¶€í„° Kafka ìŠ¤íŠ¸ë¦¬ë°ê¹Œì§€, ëª¨ë“  ë°ì´í„°ì˜ ì‹œì‘ì ì„ ë‹´ë‹¹

---

## ğŸ‘¥ íŒ€ êµ¬ì„± ë° ì—­í• 

### í˜„ì¬ íŒ€ êµ¬ì„± (Phase 1)
- **íŒ€ ë¦¬ë”**: Senior Data Engineer
- **íŒ€ ê·œëª¨**: 2ëª…
- **ì „ë¬¸ ì˜ì—­**: ì›¹ í¬ë¡¤ë§, ë°ì´í„° íŒŒì´í”„ë¼ì¸, ìŠ¤íŠ¸ë¦¬ë°

### Phase 2 í™•ì¥ ê³„íš
- **ì‹ ê·œ ì˜ì…**: Data Engineer (ë©€í‹°ì†ŒìŠ¤ ì „ë¬¸)
- **ëª©í‘œ íŒ€ ê·œëª¨**: 4ëª…
- **í™•ì¥ ì˜ì—­**: 18ê°œ ì–¸ë¡ ì‚¬, ê³µì‹œì •ë³´, ì†Œì…œë¯¸ë””ì–´

---

## ğŸ¯ í•µì‹¬ ì±…ì„ ì˜ì—­

### 1. ì›¹ í¬ë¡¤ë§ ì‹œìŠ¤í…œ
```python
# BaseCrawler ì•„í‚¤í…ì²˜ í™•ì¥
class BaseCrawler:
    """ëª¨ë“  í¬ë¡¤ëŸ¬ì˜ ê¸°ë³¸ í´ë˜ìŠ¤"""
    def crawl(self) -> List[NewsItem]
    def process_rate_limit(self)
    def handle_errors(self)
    def validate_data(self)

# í˜„ì¬ êµ¬í˜„: ì¡°ì„ ì¼ë³´
class ChosunCrawler(BaseCrawler):
    rate_limit = "5 requests/second"
    success_rate = ">99%"
    
# Phase 2 í™•ì¥: 17ê°œ ì¶”ê°€
class JoongangCrawler(BaseCrawler): pass
class DongaCrawler(BaseCrawler): pass
# ... 15ê°œ ë”
```

### 2. Kafka ìŠ¤íŠ¸ë¦¬ë° íŒŒì´í”„ë¼ì¸
```yaml
í† í”½ ì•„í‚¤í…ì²˜:
  raw-news:
    description: "ì›ì‹œ ë‰´ìŠ¤ ë°ì´í„°"
    producer: "Data Service"
    consumer: "ML Service"
    
  enriched-news:
    description: "NLP ì²˜ë¦¬ëœ ë‰´ìŠ¤"
    producer: "ML Service"  
    consumer: "Graph Service"
    
  error-news:
    description: "ì²˜ë¦¬ ì‹¤íŒ¨ ë°ì´í„°"
    producer: "ëª¨ë“  ì„œë¹„ìŠ¤"
    consumer: "Data Service (ì¬ì²˜ë¦¬)"
```

### 3. ë°ì´í„° í’ˆì§ˆ ê´€ë¦¬
```python
# ë°ì´í„° ê²€ì¦ íŒŒì´í”„ë¼ì¸
class DataQualityManager:
    def remove_duplicates(self)      # ì¤‘ë³µ ì œê±°
    def normalize_text(self)         # í…ìŠ¤íŠ¸ ì •ê·œí™”
    def validate_structure(self)     # êµ¬ì¡° ê²€ì¦
    def enrich_metadata(self)        # ë©”íƒ€ë°ì´í„° ì¶”ê°€
```

---

## ğŸ† Phase 1 ì£¼ìš” ì„±ê³¼

### âœ… ì™„ë£Œëœ ê¸°ëŠ¥
```
í¬ë¡¤ë§ ì‹œìŠ¤í…œ:
â”œâ”€â”€ âœ… ì¡°ì„ ì¼ë³´ ì‹¤ì‹œê°„ í¬ë¡¤ë§ (>99% ì„±ê³µë¥ )
â”œâ”€â”€ âœ… Rate limiting ë° ì—ëŸ¬ ì²˜ë¦¬
â”œâ”€â”€ âœ… ì¤‘ë³µ ì œê±° ì•Œê³ ë¦¬ì¦˜ (100% ì •í™•ë„)
â”œâ”€â”€ âœ… ë°ì´í„° ì •ê·œí™” íŒŒì´í”„ë¼ì¸
â””â”€â”€ âœ… Health check ë° ëª¨ë‹ˆí„°ë§

Kafka íŒŒì´í”„ë¼ì¸:
â”œâ”€â”€ âœ… ì‹¤ì‹œê°„ ë©”ì‹œì§€ ìŠ¤íŠ¸ë¦¬ë°
â”œâ”€â”€ âœ… ë©”ì‹œì§€ ìœ ì‹¤ ë°©ì§€ (0% ì†ì‹¤ë¥ )
â”œâ”€â”€ âœ… ìë™ ì¬ì²˜ë¦¬ ë¡œì§
â”œâ”€â”€ âœ… ë°±í”„ë ˆì…” í•¸ë“¤ë§
â””â”€â”€ âœ… ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ (ì§€ì—°ì‹œê°„ <50ms)
```

### ğŸ“Š ë‹¬ì„± ì„±ê³¼ ì§€í‘œ
| í•­ëª© | ëª©í‘œ | ë‹¬ì„± | ìƒíƒœ |
|------|------|------|------|
| **í¬ë¡¤ë§ ì„±ê³µë¥ ** | 95% | **>99%** | âœ… 4%â†‘ |
| **ë°ì´í„° ì²˜ë¦¬ëŸ‰** | 100/day | **1000+/day** | âœ… 10ë°°â†‘ |
| **Kafka ê°€ìš©ì„±** | 99% | **100%** | âœ… ì™„ë²½ |
| **ì—ëŸ¬ìœ¨** | <5% | **0%** | âœ… ë¬´ê²°í•¨ |

---

## ğŸ”§ ê¸°ìˆ  ìŠ¤íƒ ë° ë„êµ¬

### í˜„ì¬ ê¸°ìˆ  ìŠ¤íƒ
```yaml
ì–¸ì–´ ë° í”„ë ˆì„ì›Œí¬:
  - Python 3.11+
  - FastAPI (REST API)
  - Pydantic (ë°ì´í„° ê²€ì¦)
  - asyncio (ë¹„ë™ê¸° ì²˜ë¦¬)

í¬ë¡¤ë§ ë„êµ¬:
  - requests (HTTP í´ë¼ì´ì–¸íŠ¸)
  - BeautifulSoup4 (HTML íŒŒì‹±)
  - lxml (XML/HTML íŒŒì„œ)
  - fake-useragent (User-Agent ë¡œí…Œì´ì…˜)

ìŠ¤íŠ¸ë¦¬ë°:
  - Apache Kafka 3.x
  - kafka-python (Python í´ë¼ì´ì–¸íŠ¸)
  - avro-python3 (ë©”ì‹œì§€ ì§ë ¬í™”)

ë°ì´í„°ë² ì´ìŠ¤:
  - Redis (ìºì‹±, ì¤‘ë³µ ì²´í¬)
  - PostgreSQL (ë©”íƒ€ë°ì´í„°)

ëª¨ë‹ˆí„°ë§:
  - Prometheus (ë©”íŠ¸ë¦­)
  - Grafana (ëŒ€ì‹œë³´ë“œ)
  - ì»¤ìŠ¤í…€ Health Check
```

### Phase 2 ë„êµ¬ í™•ì¥
```yaml
ì¶”ê°€ ì˜ˆì •:
  - Scrapy (ê³ ì„±ëŠ¥ í¬ë¡¤ë§)
  - Selenium (JavaScript í˜ì´ì§€)
  - Apache Airflow (ì›Œí¬í”Œë¡œìš° ê´€ë¦¬)
  - Snowflake (ë°ì´í„° ì›¨ì–´í•˜ìš°ìŠ¤)
  - dbt (ë°ì´í„° ë³€í™˜)
```

---

## ğŸš€ ê°œë°œ ì›Œí¬í”Œë¡œìš°

### Daily ì‘ì—… íë¦„
```
09:00 - ì‹œìŠ¤í…œ ìƒíƒœ í™•ì¸
â”œâ”€â”€ Kafka í´ëŸ¬ìŠ¤í„° í—¬ìŠ¤ì²´í¬
â”œâ”€â”€ í¬ë¡¤ë§ ì„±ê³µë¥  í™•ì¸
â”œâ”€â”€ ì—ëŸ¬ ë¡œê·¸ ë¦¬ë·°
â””â”€â”€ ì„±ëŠ¥ ì§€í‘œ ëª¨ë‹ˆí„°ë§

10:00 - ê°œë°œ ì‘ì—…
â”œâ”€â”€ ì‹ ê·œ í¬ë¡¤ëŸ¬ ê°œë°œ
â”œâ”€â”€ ë°ì´í„° í’ˆì§ˆ ê°œì„ 
â”œâ”€â”€ ì„±ëŠ¥ ìµœì í™”
â””â”€â”€ í…ŒìŠ¤íŠ¸ ì½”ë“œ ì‘ì„±

14:00 - Integration Sync
â”œâ”€â”€ ML Squadì™€ ë°ì´í„° ìŠ¤í‚¤ë§ˆ í˜‘ì˜
â”œâ”€â”€ Graph Squadì™€ ë©”ì‹œì§€ í¬ë§· ë…¼ì˜
â”œâ”€â”€ Platform Squadì™€ ì¸í”„ë¼ ì´ìŠˆ ê³µìœ 
â””â”€â”€ Product Squadì™€ ìš”êµ¬ì‚¬í•­ í™•ì¸

16:00 - í’ˆì§ˆ ë³´ì¦
â”œâ”€â”€ ë‹¨ìœ„ í…ŒìŠ¤íŠ¸ ì‹¤í–‰
â”œâ”€â”€ í†µí•© í…ŒìŠ¤íŠ¸ ê²€ì¦
â”œâ”€â”€ ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬
â””â”€â”€ ë°ì´í„° í’ˆì§ˆ ê²€ì‚¬
```

### ì½”ë“œ ë¦¬ë·° ì²´í¬ë¦¬ìŠ¤íŠ¸
```python
# í¬ë¡¤ëŸ¬ ì½”ë“œ ë¦¬ë·° ê¸°ì¤€
class CrawlerReviewChecklist:
    âœ… Rate limiting êµ¬í˜„ ì—¬ë¶€
    âœ… ì—ëŸ¬ í•¸ë“¤ë§ (ë„¤íŠ¸ì›Œí¬, íŒŒì‹±)
    âœ… ë¡œê¹… ë° ëª¨ë‹ˆí„°ë§
    âœ… í…ŒìŠ¤íŠ¸ ì½”ë“œ ì‘ì„±
    âœ… ì„±ëŠ¥ ìµœì í™” (ë©”ëª¨ë¦¬, ì†ë„)
    âœ… ë³´ì•ˆ ê³ ë ¤ì‚¬í•­ (User-Agent, í”„ë¡ì‹œ)
    âœ… ë°ì´í„° ê²€ì¦ ë¡œì§
    âœ… ë¬¸ì„œí™” (README, ì£¼ì„)
```

---

## ğŸ“‹ í˜„ì¬ ì‘ì—… ë° ìš°ì„ ìˆœìœ„

### ì§„í–‰ ì¤‘ì¸ ì‘ì—…
```
Sprint ì§„í–‰ ìƒí™©:
â”œâ”€â”€ ğŸ”„ í¬ë¡¤ë§ ì„±ëŠ¥ ìµœì í™” (ì§„í–‰ì¤‘ - 80%)
â”œâ”€â”€ ğŸ”„ Kafka íŒŒí‹°ì…”ë‹ ì „ëµ ê°œì„  (ì§„í–‰ì¤‘ - 60%)
â”œâ”€â”€ ğŸ”„ ë°ì´í„° í’ˆì§ˆ ìë™í™” (ì§„í–‰ì¤‘ - 70%)
â””â”€â”€ ğŸ”„ ëª¨ë‹ˆí„°ë§ ëŒ€ì‹œë³´ë“œ êµ¬ì¶• (ì§„í–‰ì¤‘ - 90%)
```

### Phase 2 ì¤€ë¹„ ì‘ì—…
```
ì¤€ë¹„ ì¤‘ì¸ ì‘ì—…:
â”œâ”€â”€ ğŸ“‹ 17ê°œ ì–¸ë¡ ì‚¬ í¬ë¡¤ëŸ¬ ì„¤ê³„
â”œâ”€â”€ ğŸ“‹ ê³µì‹œì •ë³´ API ì—°ë™ ë°©ì•ˆ
â”œâ”€â”€ ğŸ“‹ ì†Œì…œë¯¸ë””ì–´ ë°ì´í„° ìˆ˜ì§‘ ì „ëµ
â”œâ”€â”€ ğŸ“‹ ëŒ€ìš©ëŸ‰ ì²˜ë¦¬ ì•„í‚¤í…ì²˜ ì„¤ê³„
â””â”€â”€ ğŸ“‹ Kubernetes ë°°í¬ ì¤€ë¹„
```

---

## ğŸ§ª í…ŒìŠ¤íŠ¸ ì „ëµ

### í…ŒìŠ¤íŠ¸ í”¼ë¼ë¯¸ë“œ
```
Data Squad í…ŒìŠ¤íŠ¸ êµ¬ì¡°:
â”œâ”€â”€ E2E í…ŒìŠ¤íŠ¸ (10%)
â”‚   â”œâ”€â”€ ì „ì²´ ë°ì´í„° íŒŒì´í”„ë¼ì¸ ê²€ì¦
â”‚   â””â”€â”€ ë‹¤ìš´ìŠ¤íŠ¸ë¦¼ ì„œë¹„ìŠ¤ í†µí•© í…ŒìŠ¤íŠ¸
â”œâ”€â”€ í†µí•© í…ŒìŠ¤íŠ¸ (20%)
â”‚   â”œâ”€â”€ Kafka ë©”ì‹œì§€ í”Œë¡œìš° í…ŒìŠ¤íŠ¸
â”‚   â”œâ”€â”€ ë°ì´í„°ë² ì´ìŠ¤ ì—°ë™ í…ŒìŠ¤íŠ¸
â”‚   â””â”€â”€ ì™¸ë¶€ API ì—°ë™ í…ŒìŠ¤íŠ¸
â””â”€â”€ ë‹¨ìœ„ í…ŒìŠ¤íŠ¸ (70%)
    â”œâ”€â”€ í¬ë¡¤ëŸ¬ ë¡œì§ í…ŒìŠ¤íŠ¸
    â”œâ”€â”€ ë°ì´í„° ë³€í™˜ í…ŒìŠ¤íŠ¸
    â””â”€â”€ ì—ëŸ¬ í•¸ë“¤ë§ í…ŒìŠ¤íŠ¸
```

### ìë™í™”ëœ í…ŒìŠ¤íŠ¸
```python
# í¬ë¡¤ë§ í…ŒìŠ¤íŠ¸ ìë™í™”
@pytest.mark.integration
def test_chosun_crawler():
    crawler = ChosunCrawler()
    articles = crawler.crawl()
    
    assert len(articles) > 0
    assert all(article.title for article in articles)
    assert all(article.content for article in articles)
    assert all(article.published_at for article in articles)

# Kafka ë©”ì‹œì§€ í…ŒìŠ¤íŠ¸
@pytest.mark.kafka
def test_kafka_message_flow():
    producer.send('raw-news', test_news_data)
    messages = consumer.poll(timeout_ms=5000)
    
    assert len(messages) == 1
    assert messages[0].value == test_news_data
```

---

## ğŸ“Š ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§

### í•µì‹¬ ì§€í‘œ ì¶”ì 
```yaml
í¬ë¡¤ë§ ì§€í‘œ:
  - ì„±ê³µë¥ : >99% (ëª©í‘œ: >95%)
  - ì‘ë‹µì‹œê°„: <2ì´ˆ (ëª©í‘œ: <5ì´ˆ)
  - ì‹œê°„ë‹¹ ê¸°ì‚¬ ìˆ˜: 100+ (ëª©í‘œ: 50+)
  - ì—ëŸ¬ìœ¨: 0% (ëª©í‘œ: <5%)

Kafka ì§€í‘œ:
  - ë©”ì‹œì§€ ì²˜ë¦¬ëŸ‰: 1000+ msg/s
  - ì§€ì—°ì‹œê°„: <50ms (P95)
  - íŒŒí‹°ì…˜ ë°¸ëŸ°ìŠ¤: Â±5%
  - ì¬ì²˜ë¦¬ìœ¨: <1%

ë°ì´í„° í’ˆì§ˆ:
  - ì¤‘ë³µë¥ : 0% (ì™„ë²½í•œ ì¤‘ë³µ ì œê±°)
  - ëˆ„ë½ í•„ë“œ: 0%
  - ìŠ¤í‚¤ë§ˆ ì¤€ìˆ˜ìœ¨: 100%
  - ì •ê·œí™” ì •í™•ë„: 100%
```

### ì•Œë¦¼ ë° ëŒ€ì‘
```yaml
ì•Œë¦¼ ì„¤ì •:
  í¬ë¦¬í‹°ì»¬:
    - í¬ë¡¤ë§ ì‹¤íŒ¨ìœ¨ >5%
    - Kafka ë©”ì‹œì§€ ìœ ì‹¤
    - ì‹œìŠ¤í…œ ë‹¤ìš´
    ëŒ€ì‘: ì¦‰ì‹œ ì•Œë¦¼ + ìë™ ë³µêµ¬ ì‹œë„

ê²½ê³ :
    - ì‘ë‹µì‹œê°„ >5ì´ˆ
    - ë©”ëª¨ë¦¬ ì‚¬ìš©ë¥  >80%
    - ë””ìŠ¤í¬ ì‚¬ìš©ë¥  >85%
    ëŒ€ì‘: ìŠ¬ë™ ì•Œë¦¼ + ëª¨ë‹ˆí„°ë§ ê°•í™”
```

---

## ğŸ› ï¸ íŠ¸ëŸ¬ë¸”ìŠˆíŒ… ê°€ì´ë“œ

### ìì£¼ ë°œìƒí•˜ëŠ” ì´ìŠˆ

#### 1. í¬ë¡¤ë§ ì‹¤íŒ¨
```python
ë¬¸ì œ: HTTP 403/429 ì—ëŸ¬
ì›ì¸: Rate limiting ë˜ëŠ” IP ì°¨ë‹¨
í•´ê²°:
  1. User-Agent ë¡œí…Œì´ì…˜ í™•ì¸
  2. ìš”ì²­ ê°„ê²© ì¦ê°€
  3. í”„ë¡ì‹œ ì„œë²„ ì‚¬ìš©
  4. í¬ë¡¤ë§ ì‹œê°„ ë¶„ì‚°

# êµ¬í˜„ ì˜ˆì‹œ
class RateLimiter:
    def __init__(self, requests_per_second=2):
        self.min_interval = 1.0 / requests_per_second
        self.last_request = 0
    
    def wait_if_needed(self):
        elapsed = time.time() - self.last_request
        if elapsed < self.min_interval:
            time.sleep(self.min_interval - elapsed)
        self.last_request = time.time()
```

#### 2. Kafka ë©”ì‹œì§€ ìœ ì‹¤
```python
ë¬¸ì œ: ë©”ì‹œì§€ê°€ Consumerì— ë„ë‹¬í•˜ì§€ ì•ŠìŒ
ì›ì¸: Producer ì„¤ì • ë¬¸ì œ ë˜ëŠ” ë„¤íŠ¸ì›Œí¬ ì´ìŠˆ
í•´ê²°:
  1. acks='all' ì„¤ì • í™•ì¸
  2. retries ì„¤ì • ì¦ê°€
  3. ë„¤íŠ¸ì›Œí¬ ì—°ê²° ìƒíƒœ ì ê²€
  4. Consumer group ìƒíƒœ í™•ì¸

# ì•ˆì „í•œ Producer ì„¤ì •
producer_config = {
    'acks': 'all',
    'retries': 3,
    'retry_backoff_ms': 1000,
    'request_timeout_ms': 30000
}
```

#### 3. ë©”ëª¨ë¦¬ ë¶€ì¡±
```python
ë¬¸ì œ: í¬ë¡¤ë§ ì¤‘ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ê¸‰ì¦
ì›ì¸: ëŒ€ìš©ëŸ‰ í˜ì´ì§€ ë˜ëŠ” ë©”ëª¨ë¦¬ ëˆ„ìˆ˜
í•´ê²°:
  1. ìŠ¤íŠ¸ë¦¬ë° íŒŒì‹± ì‚¬ìš©
  2. ì •ê¸°ì ì¸ ê°€ë¹„ì§€ ì»¬ë ‰ì…˜
  3. ë°°ì¹˜ í¬ê¸° ì¡°ì •
  4. ë©”ëª¨ë¦¬ í”„ë¡œíŒŒì¼ë§

# ë©”ëª¨ë¦¬ íš¨ìœ¨ì ì¸ íŒŒì‹±
def parse_large_page(content):
    # í•œ ë²ˆì— ëª¨ë“  ë‚´ìš©ì„ ë©”ëª¨ë¦¬ì— ë¡œë“œí•˜ì§€ ì•ŠìŒ
    soup = BeautifulSoup(content, 'lxml')
    for article in soup.find_all('article'):
        yield parse_article(article)
        article.decompose()  # ë©”ëª¨ë¦¬ í•´ì œ
```

---

## ğŸ“š í•™ìŠµ ë¦¬ì†ŒìŠ¤

### í•„ìˆ˜ í•™ìŠµ ìë£Œ
```
ì›¹ í¬ë¡¤ë§:
â”œâ”€â”€ "Web Scraping with Python" (O'Reilly)
â”œâ”€â”€ Scrapy ê³µì‹ ë¬¸ì„œ
â”œâ”€â”€ HTTP í”„ë¡œí† ì½œ ì™„ë²½ ê°€ì´ë“œ
â””â”€â”€ robots.txt ë° í¬ë¡¤ë§ ìœ¤ë¦¬

Apache Kafka:
â”œâ”€â”€ "Kafka: The Definitive Guide" (O'Reilly)
â”œâ”€â”€ Confluent Kafka íŠœí† ë¦¬ì–¼
â”œâ”€â”€ ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¬ë° ì•„í‚¤í…ì²˜ íŒ¨í„´
â””â”€â”€ Kafka ì„±ëŠ¥ íŠœë‹ ê°€ì´ë“œ

ë°ì´í„° ì—”ì§€ë‹ˆì–´ë§:
â”œâ”€â”€ "Designing Data-Intensive Applications"
â”œâ”€â”€ Apache Airflow ë§ˆìŠ¤í„° í´ë˜ìŠ¤
â”œâ”€â”€ ë°ì´í„° í’ˆì§ˆ ê´€ë¦¬ ë² ìŠ¤íŠ¸ í”„ë™í‹°ìŠ¤
â””â”€â”€ ì‹¤ì‹œê°„ ë°ì´í„° íŒŒì´í”„ë¼ì¸ ì„¤ê³„
```

### ì¸ì¦ ë° êµìœ¡
```
ì¶”ì²œ ì¸ì¦:
â”œâ”€â”€ AWS Certified Data Engineer
â”œâ”€â”€ Apache Kafka ê°œë°œì ì¸ì¦
â”œâ”€â”€ Python ê³ ê¸‰ ë°ì´í„° ì²˜ë¦¬
â””â”€â”€ Kubernetes ì• í”Œë¦¬ì¼€ì´ì…˜ ê°œë°œì
```

---

## ğŸ¯ ì»¤ë¦¬ì–´ ê°œë°œ ê²½ë¡œ

### ìŠ¤í‚¬ ê°œë°œ ë¡œë“œë§µ
```
Junior Data Engineer â†’ Senior Data Engineer:
â”œâ”€â”€ Month 1-3: Python ë§ˆìŠ¤í„°ë¦¬
â”œâ”€â”€ Month 4-6: Kafka ì „ë¬¸ê°€
â”œâ”€â”€ Month 7-9: í´ë¼ìš°ë“œ í”Œë«í¼ (AWS/GCP)
â”œâ”€â”€ Month 10-12: ë¶„ì‚° ì‹œìŠ¤í…œ ì„¤ê³„
â””â”€â”€ Year 2: íŒ€ ë¦¬ë”ì‹­ ë° ì•„í‚¤í…íŠ¸

ì „ë¬¸ ë¶„ì•¼ ì„ íƒ:
â”œâ”€â”€ ğŸš€ ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¬ë° ì „ë¬¸ê°€
â”œâ”€â”€ ğŸŒ ì›¹ í¬ë¡¤ë§ ë° API ì „ë¬¸ê°€
â”œâ”€â”€ ğŸ—ï¸ ë°ì´í„° ì•„í‚¤í…íŠ¸
â””â”€â”€ ğŸ“Š ë°ì´í„° í”Œë«í¼ ì—”ì§€ë‹ˆì–´
```

### ì„±ê³¼ í‰ê°€ ê¸°ì¤€
```
ê¸°ìˆ  ì—­ëŸ‰ (50%):
â”œâ”€â”€ ì½”ë“œ í’ˆì§ˆ ë° í…ŒìŠ¤íŠ¸ ì»¤ë²„ë¦¬ì§€
â”œâ”€â”€ ì‹œìŠ¤í…œ ì„±ëŠ¥ ìµœì í™” ê¸°ì—¬ë„
â”œâ”€â”€ ìƒˆë¡œìš´ ê¸°ìˆ  ë„ì… ë° ì ìš©
â””â”€â”€ ë¬¸ì œ í•´ê²° ëŠ¥ë ¥

í˜‘ì—… ë° ì»¤ë®¤ë‹ˆì¼€ì´ì…˜ (30%):
â”œâ”€â”€ ë‹¤ë¥¸ Squadì™€ì˜ í˜‘ì—… í’ˆì§ˆ
â”œâ”€â”€ ê¸°ìˆ  ì§€ì‹ ê³µìœ  ë° ë¬¸ì„œí™”
â”œâ”€â”€ ì½”ë“œ ë¦¬ë·° ì°¸ì—¬ë„
â””â”€â”€ ë©˜í† ë§ ë° ì§€ì‹ ì „íŒŒ

ë¹„ì¦ˆë‹ˆìŠ¤ ì„íŒ©íŠ¸ (20%):
â”œâ”€â”€ ë°ì´í„° í’ˆì§ˆ ê°œì„  ê¸°ì—¬ë„
â”œâ”€â”€ ì‹œìŠ¤í…œ ì•ˆì •ì„± í–¥ìƒ
â”œâ”€â”€ ê°œë°œ ìƒì‚°ì„± ì¦ëŒ€
â””â”€â”€ ê³ ê° ë§Œì¡±ë„ ê°œì„ 
```

---

## ğŸ”® Phase 2 ì¤€ë¹„ì‚¬í•­

### ê¸°ìˆ ì  ì¤€ë¹„
```
ë©€í‹°ì†ŒìŠ¤ í¬ë¡¤ë§:
â”œâ”€â”€ 17ê°œ ì–¸ë¡ ì‚¬ API ì¡°ì‚¬ ì™„ë£Œ
â”œâ”€â”€ ê³µì‹œì •ë³´ API ì—°ë™ ë°©ì•ˆ ì„¤ê³„
â”œâ”€â”€ ì†Œì…œë¯¸ë””ì–´ ë°ì´í„° ìˆ˜ì§‘ ì „ëµ
â””â”€â”€ ëŒ€ìš©ëŸ‰ ì²˜ë¦¬ ì•„í‚¤í…ì²˜ ì„¤ê³„

ì¸í”„ë¼ í™•ì¥:
â”œâ”€â”€ Kubernetes ë§ˆì´ê·¸ë ˆì´ì…˜ ê³„íš
â”œâ”€â”€ Kafka í´ëŸ¬ìŠ¤í„° ìŠ¤ì¼€ì¼ë§
â”œâ”€â”€ ìŠ¤í† ë¦¬ì§€ í™•ì¥ (Data Lake)
â””â”€â”€ ëª¨ë‹ˆí„°ë§ ì‹œìŠ¤í…œ ê³ ë„í™”
```

### íŒ€ í™•ì¥ ì¤€ë¹„
```
ì‹ ê·œ íŒ€ì› ì˜¨ë³´ë”©:
â”œâ”€â”€ ì˜¨ë³´ë”© ê°€ì´ë“œ ë¬¸ì„œ ì‘ì„±
â”œâ”€â”€ ê°œë°œ í™˜ê²½ ìë™í™” ìŠ¤í¬ë¦½íŠ¸
â”œâ”€â”€ ë©˜í† ë§ í”„ë¡œê·¸ë¨ ì„¤ê³„
â””â”€â”€ ì½”ë“œ ë¦¬ë·° í”„ë¡œì„¸ìŠ¤ ì •ë¦½
```

---

## ğŸ“ ë„ì›€ì´ í•„ìš”í•  ë•Œ

### íŒ€ ë‚´ ì—°ë½ì²˜
```
íŒ€ ë¦¬ë”: @data-lead (Slack)
ì‹œë‹ˆì–´ ì—”ì§€ë‹ˆì–´: @senior-data-eng
DevOps ì§€ì›: @platform-squad
ê¸´ê¸‰ ìƒí™©: #riskradar-alerts
```

### ì™¸ë¶€ ë¦¬ì†ŒìŠ¤
```
ê¸°ìˆ  ì§€ì›:
â”œâ”€â”€ Kafka ì»¤ë®¤ë‹ˆí‹° í¬ëŸ¼
â”œâ”€â”€ Python ë°ì´í„° ì—”ì§€ë‹ˆì–´ë§ ê·¸ë£¹
â”œâ”€â”€ AWS/GCP ê¸°ìˆ  ì§€ì›
â””â”€â”€ Stack Overflow (riskradar íƒœê·¸)
```

---

*ìµœì¢… ì—…ë°ì´íŠ¸: 2025-07-19*