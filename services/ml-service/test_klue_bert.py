#!/usr/bin/env python3
"""
KLUE-BERT NER ëª¨ë¸ í…ŒìŠ¤íŠ¸
Test KLUE-BERT NER Model
"""
import asyncio
import sys
import os
import json

# Add src to path
sys.path.append(os.path.join(os.path.dirname(__file__), 'src'))

from src.models.ner.klue_bert_ner import KLUEBERTNERModel
from src.evaluation.f1_score_evaluator import F1ScoreEvaluator


async def test_klue_bert_model():
    """KLUE-BERT ëª¨ë¸ í…ŒìŠ¤íŠ¸"""
    print("ğŸ§ª Testing KLUE-BERT NER Model...")
    
    # ëª¨ë¸ ì´ˆê¸°í™”
    model = KLUEBERTNERModel()
    
    # ëª¨ë¸ ì •ë³´ ì¶œë ¥
    model_info = model.get_model_info()
    print(f"\nğŸ“Š Model Information:")
    for key, value in model_info.items():
        print(f"  {key}: {value}")
    
    # ê°„ë‹¨í•œ í…ŒìŠ¤íŠ¸
    test_texts = [
        "ì‚¼ì„±ì „ìê°€ ìƒˆë¡œìš´ ë°˜ë„ì²´ ê³µì¥ì„ ê±´ì„¤í•œë‹¤ê³  ì´ì¬ìš© íšŒì¥ì´ ë°œí‘œí–ˆìŠµë‹ˆë‹¤.",
        "í˜„ëŒ€ìë™ì°¨ ì •ì˜ì„  íšŒì¥ì€ ì „ê¸°ì°¨ ì‹œì¥ ì§„ì¶œì„ ì„ ì–¸í–ˆë‹¤.",
        "ë„¤ì´ë²„ ìµœìˆ˜ì—° ëŒ€í‘œê°€ AI í”Œë«í¼ ê³ ë„í™” ê³„íšì„ ë°œí‘œí–ˆë‹¤.",
        "KBê¸ˆìœµ ìœ¤ì¢…ê·œ íšŒì¥ê³¼ ì‹ í•œê¸ˆìœµ ì§„ì˜¥ë™ íšŒì¥ì´ í•€í…Œí¬ í˜‘ë ¥ì„ ë…¼ì˜í–ˆë‹¤."
    ]
    
    print(f"\nğŸ” Individual Test Results:")
    
    for i, text in enumerate(test_texts, 1):
        print(f"\n  Test {i}: '{text}'")
        
        try:
            entities = await model.extract_entities(text)
            print(f"    Found {len(entities)} entities:")
            
            for entity in entities:
                print(f"      - {entity.text} ({entity.type}) [confidence: {entity.confidence:.2f}]")
                
        except Exception as e:
            print(f"    âŒ Error: {e}")
    
    return model


async def evaluate_klue_bert_performance():
    """KLUE-BERT ì„±ëŠ¥ í‰ê°€"""
    print(f"\nğŸ“ˆ KLUE-BERT Performance Evaluation...")
    
    # ëª¨ë¸ ì´ˆê¸°í™”
    model = KLUEBERTNERModel()
    
    # í‰ê°€ê¸° ì´ˆê¸°í™”
    evaluator = F1ScoreEvaluator()
    
    # í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ ê²½ë¡œ
    test_dataset_path = "tests/test_data/ner_test_dataset.json"
    
    if not os.path.exists(test_dataset_path):
        print(f"âŒ Test dataset not found: {test_dataset_path}")
        return None
    
    try:
        # ëª¨ë¸ í‰ê°€ ì‹¤í–‰
        async def model_func(text):
            return await model.extract_entities(text)
        
        evaluation = evaluator.evaluate_model(
            lambda text: asyncio.run(model_func(text)),
            test_dataset_path
        )
        
        # ê²°ê³¼ ì¶œë ¥
        evaluator.print_evaluation_report(evaluation)
        
        return evaluation
        
    except Exception as e:
        print(f"âŒ Evaluation failed: {e}")
        return None


def compare_with_previous_models():
    """ì´ì „ ëª¨ë¸ë“¤ê³¼ ì„±ëŠ¥ ë¹„êµ"""
    print(f"\nğŸ“Š Performance Comparison:")
    
    previous_results = {
        "Mock NER Model": 32.3,
        "Multilingual NER (API)": 37.8,
        "KoELECTRA Naver NER": 56.3
    }
    
    print(f"  Previous Models:")
    for model_name, f1_score in previous_results.items():
        print(f"    {model_name}: {f1_score}% F1")
    
    print(f"\n  ğŸ¯ Target F1-Score: 80.0%")
    print(f"  ğŸ“ˆ Expected improvement with KLUE-BERT: +15-20%")


async def test_model_features():
    """ëª¨ë¸ ê¸°ëŠ¥ í…ŒìŠ¤íŠ¸"""
    print(f"\nğŸ”§ Testing Model Features...")
    
    model = KLUEBERTNERModel()
    
    # ìºì‹œ í…ŒìŠ¤íŠ¸
    test_text = "ì‚¼ì„±ì „ì ì´ì¬ìš© íšŒì¥ì´ ìƒˆë¡œìš´ ì „ëµì„ ë°œí‘œí–ˆë‹¤."
    
    print(f"  Testing caching...")
    # ì²« ë²ˆì§¸ í˜¸ì¶œ (ìºì‹œ ë¯¸ìŠ¤)
    start_time = asyncio.get_event_loop().time()
    entities1 = await model.extract_entities(test_text)
    time1 = (asyncio.get_event_loop().time() - start_time) * 1000
    
    # ë‘ ë²ˆì§¸ í˜¸ì¶œ (ìºì‹œ íˆíŠ¸)
    start_time = asyncio.get_event_loop().time()
    entities2 = await model.extract_entities(test_text)
    time2 = (asyncio.get_event_loop().time() - start_time) * 1000
    
    print(f"    First call: {time1:.2f}ms ({len(entities1)} entities)")
    print(f"    Second call: {time2:.2f}ms ({len(entities2)} entities)")
    print(f"    Speed improvement: {time1/time2:.1f}x")
    
    # ìºì‹œ í†µê³„
    cache_stats = model.get_cache_stats()
    print(f"  Cache statistics:")
    for key, value in cache_stats.items():
        print(f"    {key}: {value}")


async def main():
    """ë©”ì¸ ì‹¤í–‰"""
    print("="*60)
    print("KLUE-BERT NER Model Testing - Week 3")
    print("="*60)
    
    # ëª¨ë¸ ê¸°ë³¸ í…ŒìŠ¤íŠ¸
    model = await test_klue_bert_model()
    
    # ì„±ëŠ¥ í‰ê°€
    evaluation = await evaluate_klue_bert_performance()
    
    # ì´ì „ ëª¨ë¸ê³¼ ë¹„êµ
    compare_with_previous_models()
    
    # ëª¨ë¸ ê¸°ëŠ¥ í…ŒìŠ¤íŠ¸
    await test_model_features()
    
    print("\n" + "="*60)
    
    if evaluation:
        current_f1 = evaluation['overall']['f1'] * 100
        target_f1 = 80.0
        
        if current_f1 >= target_f1:
            print(f"ğŸ‰ SUCCESS: Target F1-Score achieved! ({current_f1:.1f}% >= {target_f1}%)")
        else:
            improvement_needed = target_f1 - current_f1
            print(f"ğŸ“ˆ PROGRESS: F1-Score improved, {improvement_needed:.1f}% more needed")
            print(f"   Current: {current_f1:.1f}%")
            print(f"   Target: {target_f1}%")
    else:
        print("âš ï¸  Evaluation incomplete - check model implementation")
    
    print("="*60)


if __name__ == "__main__":
    asyncio.run(main())