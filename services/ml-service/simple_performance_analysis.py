#!/usr/bin/env python3
"""
ê°„ë‹¨í•œ NER ì„±ëŠ¥ ë¶„ì„
Simple NER Performance Analysis
"""
import json
import sys
import os
from typing import List, Dict

# Test dataset ë¡œë“œ
def load_test_dataset():
    """í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ ë¡œë“œ"""
    with open('tests/test_data/ner_test_dataset.json', 'r', encoding='utf-8') as f:
        data = json.load(f)
    return data['test_cases']

def analyze_dataset_characteristics():
    """ë°ì´í„°ì…‹ íŠ¹ì„± ë¶„ì„"""
    test_cases = load_test_dataset()
    
    print("ğŸ“Š Test Dataset Analysis:")
    print(f"  Total test cases: {len(test_cases)}")
    
    # Entity type distribution
    entity_counts = {"COMPANY": 0, "PERSON": 0, "EVENT": 0}
    total_entities = 0
    
    for case in test_cases:
        for entity in case['expected_entities']:
            entity_type = entity['type']
            if entity_type in entity_counts:
                entity_counts[entity_type] += 1
                total_entities += 1
    
    print(f"  Total entities: {total_entities}")
    print(f"  Entity distribution:")
    for entity_type, count in entity_counts.items():
        percentage = (count / total_entities) * 100 if total_entities > 0 else 0
        print(f"    {entity_type}: {count} ({percentage:.1f}%)")
    
    return test_cases, entity_counts

def identify_challenging_cases():
    """ì–´ë ¤ìš´ ì¼€ì´ìŠ¤ ì‹ë³„"""
    test_cases = load_test_dataset()
    
    print(f"\nğŸ¯ Challenging Cases Analysis:")
    
    # ë³µí•© ì—”í‹°í‹° ì¼€ì´ìŠ¤ (ì—¬ëŸ¬ íšŒì‚¬ëª…ì´ ì—°ê²°ëœ ê²½ìš°)
    compound_cases = []
    multiple_entities = []
    
    for case in test_cases:
        entities = case['expected_entities']
        if len(entities) > 2:
            multiple_entities.append(case)
        
        # ì—°ê²°ì‚¬ê°€ í¬í•¨ëœ í…ìŠ¤íŠ¸ ì°¾ê¸°
        text = case['text']
        conjunctions = ['ì™€', 'ê³¼', 'ì´', 'ê°€', 'ì€', 'ëŠ”']
        if any(conj in text for conj in conjunctions):
            compound_cases.append(case)
    
    print(f"  Multiple entities cases: {len(multiple_entities)}")
    print(f"  Compound cases (conjunctions): {len(compound_cases)}")
    
    # ì˜ˆì‹œ ì¶œë ¥
    print(f"\n  ğŸ“ Example challenging cases:")
    for i, case in enumerate(multiple_entities[:3]):
        print(f"    {i+1}. '{case['text']}'")
        print(f"       Expected: {len(case['expected_entities'])} entities")

def analyze_current_f1_score():
    """í˜„ì¬ F1 ì ìˆ˜ ë¶„ì„"""
    print(f"\nğŸ“ˆ Current Performance Analysis:")
    print(f"  Previous Week 2 Results:")
    print(f"    Mock Model F1: 32.3%")
    print(f"    Multilingual Model F1: 37.8%") 
    print(f"    KoELECTRA Naver F1: 56.3%")
    
    print(f"\n  ğŸ¯ Goal Analysis:")
    current_best = 56.3
    target = 80.0
    improvement_needed = target - current_best
    
    print(f"    Current Best: {current_best}%")
    print(f"    Target: {target}%")
    print(f"    Improvement Needed: {improvement_needed}% (gap: {improvement_needed/target*100:.1f}%)")
    
    return current_best, target, improvement_needed

def suggest_improvement_strategy():
    """ê°œì„  ì „ëµ ì œì•ˆ"""
    print(f"\nğŸ’¡ Week 3 Improvement Strategy:")
    
    print(f"\n  ğŸš€ Priority 1: Korean-Specific Model")
    print(f"    - Implement KLUE-BERT NER model")
    print(f"    - Expected improvement: +15-20% F1")
    print(f"    - Reason: Better Korean language understanding")
    
    print(f"\n  ğŸ”§ Priority 2: Enhanced Post-processing")
    print(f"    - Improve company name normalization")
    print(f"    - Better handling of conjunctions")
    print(f"    - Expected improvement: +5-10% F1")
    
    print(f"\n  ğŸ“š Priority 3: Domain-Specific Fine-tuning")
    print(f"    - Create financial news training dataset")
    print(f"    - Fine-tune KLUE-BERT on domain data")
    print(f"    - Expected improvement: +5-15% F1")
    
    print(f"\n  ğŸ¯ Combined Expected F1: 76-91% (Target: 80%)")

def create_week3_roadmap():
    """Week 3 ë¡œë“œë§µ ìƒì„±"""
    print(f"\nğŸ—“ï¸ Week 3 Development Roadmap:")
    
    roadmap = [
        ("Day 1-2", "KLUE-BERT NER Model Implementation", "High"),
        ("Day 3", "Model Integration & Testing", "High"), 
        ("Day 4", "Enhanced Post-processing Rules", "Medium"),
        ("Day 5", "Domain Dataset Collection", "Medium"),
        ("Day 6", "Fine-tuning Experiment", "Low"),
        ("Day 7", "Performance Evaluation & Documentation", "High")
    ]
    
    for day, task, priority in roadmap:
        print(f"    {day}: {task} [{priority} Priority]")

def main():
    """ë©”ì¸ ì‹¤í–‰"""
    print("=" * 60)
    print("ML Service Week 3 Performance Analysis")
    print("=" * 60)
    
    # ë°ì´í„°ì…‹ íŠ¹ì„± ë¶„ì„
    test_cases, entity_counts = analyze_dataset_characteristics()
    
    # ì–´ë ¤ìš´ ì¼€ì´ìŠ¤ ì‹ë³„
    identify_challenging_cases()
    
    # í˜„ì¬ ì„±ëŠ¥ ë¶„ì„
    current_best, target, improvement_needed = analyze_current_f1_score()
    
    # ê°œì„  ì „ëµ ì œì•ˆ
    suggest_improvement_strategy()
    
    # Week 3 ë¡œë“œë§µ
    create_week3_roadmap()
    
    print("=" * 60)
    print("Analysis Complete! Ready to start KLUE-BERT implementation")
    print("=" * 60)

if __name__ == "__main__":
    main()