"""Graph Service Main Application"""
from fastapi import FastAPI, HTTPException, Response
from fastapi.responses import HTMLResponse
from contextlib import asynccontextmanager
import os
import logging
from datetime import datetime
from typing import Dict, Any, List, Optional

from src.neo4j.driver import driver as neo4j_driver
from src.kafka.consumer import consumer as kafka_consumer
from src.models.schemas import (
    GraphStatsResponse, 
    GraphQueryRequest,
    NetworkRiskRequest,
    NetworkAnalysisResponse
)
from src.kafka.entity_cache import entity_cache
from src.graphql.server import get_graphql_router, GRAPHQL_PLAYGROUND_HTML
from src.queries.optimized import get_optimized_queries
from src.queries.performance_tuning import get_performance_tuner

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=getattr(logging, os.getenv("LOG_LEVEL", "INFO")),
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

@asynccontextmanager
async def lifespan(app: FastAPI):
    """ì• í”Œë¦¬ì¼€ì´ì…˜ ìƒëª…ì£¼ê¸° ê´€ë¦¬"""
    # Startup
    logger.info("Starting Graph Service...")
    
    # Neo4j ì—°ê²° í™•ì¸ (ì¬ì‹œë„ ë¡œì§ í¬í•¨)
    import time
    max_retries = 30
    retry_interval = 2
    
    for attempt in range(max_retries):
        try:
            neo4j_driver.driver.verify_connectivity()
            logger.info("Connected to Neo4j")
            break
        except Exception as e:
            if attempt < max_retries - 1:
                logger.warning(f"Neo4j connection attempt {attempt + 1}/{max_retries} failed: {e}. Retrying in {retry_interval}s...")
                time.sleep(retry_interval)
            else:
                logger.error(f"Failed to connect to Neo4j after {max_retries} attempts: {e}")
                raise e
    
    # Kafka Consumer ì‹œì‘
    kafka_consumer.start()
    
    yield
    
    # Shutdown
    logger.info("Shutting down Graph Service...")
    kafka_consumer.stop()
    neo4j_driver.close()

# FastAPI ì•± ìƒì„±
app = FastAPI(
    title=os.getenv("API_TITLE", "RiskRadar Graph Service API"),
    version=os.getenv("API_VERSION", "1.0.0"),
    description="""
    ## RiskRadar Graph Service API
    
    Neo4j ê¸°ë°˜ Risk Knowledge Graph ì„œë¹„ìŠ¤ì…ë‹ˆë‹¤.
    
    ### ì£¼ìš” ê¸°ëŠ¥
    - ğŸ•¸ï¸ **ê·¸ë˜í”„ ë°ì´í„° ê´€ë¦¬**: ê¸°ì—…, ì¸ë¬¼, ì´ë²¤íŠ¸ ê°„ ë³µì¡í•œ ê´€ê³„ ì €ì¥
    - ğŸ“Š **ë¦¬ìŠ¤í¬ ë¶„ì„**: ë„¤íŠ¸ì›Œí¬ ê¸°ë°˜ ë¦¬ìŠ¤í¬ ì „íŒŒ ë° ì˜í–¥ë„ ë¶„ì„
    - ğŸ” **ê³ ê¸‰ ì¿¼ë¦¬**: GraphQLê³¼ REST APIë¥¼ í†µí•œ ìœ ì—°í•œ ë°ì´í„° ì¡°íšŒ
    - âš¡ **ê³ ì„±ëŠ¥**: ìµœì í™”ëœ Cypher ì¿¼ë¦¬ì™€ ìºì‹±ìœ¼ë¡œ ë¹ ë¥¸ ì‘ë‹µ
    - ğŸ“ˆ **ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§**: ì‹œìŠ¤í…œ ì„±ëŠ¥ ë° í—¬ìŠ¤ ìƒíƒœ ëª¨ë‹ˆí„°ë§
    
    ### Performance Goals
    - 1-hop ì¿¼ë¦¬: < 50ms
    - 3-hop ê²½ë¡œ ë¶„ì„: < 200ms
    - ë™ì‹œ ì²˜ë¦¬: 100+ concurrent queries
    
    ### ê´€ë ¨ ë¬¸ì„œ
    - [GraphQL Playground](/playground)
    - [Monitoring Dashboard](/monitoring/dashboard)
    - [Performance Metrics](/monitoring/metrics/summary)
    """,
    contact={
        "name": "Graph Squad",
        "email": "graph-team@riskradar.com"
    },
    license_info={
        "name": "RiskRadar License",
        "url": "https://riskradar.com/license"
    },
    servers=[
        {
            "url": "http://localhost:8003",
            "description": "Development server"
        },
        {
            "url": "https://api.riskradar.com/graph",
            "description": "Production server"
        }
    ],
    lifespan=lifespan,
    docs_url="/docs",
    redoc_url="/redoc",
    openapi_url="/openapi.json"
)

# GraphQL ë¼ìš°í„° ì¶”ê°€
graphql_router = get_graphql_router()
app.include_router(graphql_router, prefix="/graphql")

# ëª¨ë‹ˆí„°ë§ ëŒ€ì‹œë³´ë“œ ë¼ìš°í„° ì¶”ê°€
from src.monitoring.dashboard import get_monitoring_router
monitoring_router = get_monitoring_router()
app.include_router(monitoring_router)

@app.get("/playground", response_class=HTMLResponse)
async def graphql_playground():
    """GraphQL Playground ì¸í„°í˜ì´ìŠ¤"""
    return GRAPHQL_PLAYGROUND_HTML

@app.get("/health")
async def health_check():
    """í—¬ìŠ¤ ì²´í¬ ì—”ë“œí¬ì¸íŠ¸"""
    from src.monitoring.health_check import health_checker, HealthStatus
    
    try:
        # í†µí•© í—¬ìŠ¤ ì²´í¬ ì‚¬ìš©
        health_status = await health_checker.get_health_status()
        
        # ìƒíƒœì— ë”°ë¥¸ HTTP ì‘ë‹µ ì½”ë“œ ê²°ì •
        status = health_status.get("status", HealthStatus.UNKNOWN)
        
        if status == HealthStatus.CRITICAL:
            raise HTTPException(status_code=503, detail=health_status)
        elif status == HealthStatus.DEGRADED:
            # DegradedëŠ” 200ì„ ë°˜í™˜í•˜ë˜ ìƒíƒœë¥¼ ëª…ì‹œ
            return health_status
        else:
            return health_status
            
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error in health check: {e}")
        error_response = {
            "status": HealthStatus.UNKNOWN,
            "error": str(e),
            "timestamp": datetime.now().isoformat()
        }
        raise HTTPException(status_code=503, detail=error_response)

@app.get("/api/v1/graph/stats", response_model=GraphStatsResponse)
async def get_graph_stats():
    """ê·¸ë˜í”„ í†µê³„ ì¡°íšŒ"""
    try:
        # ë…¸ë“œ ìˆ˜ ì¡°íšŒ
        node_query = """
        MATCH (n)
        RETURN labels(n)[0] as type, count(n) as count
        """
        node_results = neo4j_driver.execute_read(node_query)
        
        # ê´€ê³„ ìˆ˜ ì¡°íšŒ
        rel_query = """
        MATCH ()-[r]->()
        RETURN type(r) as type, count(r) as count
        """
        rel_results = neo4j_driver.execute_read(rel_query)
        
        # ê²°ê³¼ ì§‘ê³„
        node_types = {row['type']: row['count'] for row in node_results if row['type']}
        rel_types = {row['type']: row['count'] for row in rel_results}
        
        total_nodes = sum(node_types.values())
        total_rels = sum(rel_types.values())
        
        return GraphStatsResponse(
            node_count=total_nodes,
            relationship_count=total_rels,
            node_types=node_types,
            relationship_types=rel_types,
            timestamp=datetime.now()
        )
        
    except Exception as e:
        logger.error(f"Error getting graph stats: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/api/v1/graph/query")
async def query_graph(request: GraphQueryRequest):
    """ê·¸ë˜í”„ ì¿¼ë¦¬ ì‹¤í–‰"""
    try:
        query = """
        MATCH path = (n {id: $entity_id})-[*1..$depth]-()
        RETURN path
        LIMIT 100
        """
        
        results = neo4j_driver.execute_read(
            query,
            entity_id=request.entity_id,
            depth=request.depth
        )
        
        return {
            "entity_id": request.entity_id,
            "paths": len(results),
            "results": results
        }
        
    except Exception as e:
        logger.error(f"Error querying graph: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/api/v1/graph/network-risk", response_model=NetworkAnalysisResponse)
async def analyze_network_risk(request: NetworkRiskRequest):
    """ë„¤íŠ¸ì›Œí¬ ë¦¬ìŠ¤í¬ ë¶„ì„"""
    try:
        # ì—°ê²°ëœ ê³ ìœ„í—˜ ì—”í‹°í‹° ì°¾ê¸°
        risk_query = """
        MATCH (c:Company {id: $company_id})
        MATCH path = (c)-[*1..$depth]-(connected)
        WHERE connected.risk_score >= $min_risk_score
        WITH c, connected, path
        RETURN 
            c.name as central_entity,
            count(DISTINCT connected) as connected_count,
            avg(connected.risk_score) as avg_risk,
            collect(DISTINCT {
                id: connected.id,
                name: connected.name,
                risk_score: connected.risk_score,
                type: labels(connected)[0]
            }) as high_risk_entities
        """
        
        results = neo4j_driver.execute_read(
            risk_query,
            company_id=request.company_id,
            depth=request.depth,
            min_risk_score=request.min_risk_score
        )
        
        if not results:
            raise HTTPException(status_code=404, detail="Company not found")
        
        result = results[0]
        
        return NetworkAnalysisResponse(
            central_entity=result['central_entity'],
            connected_entities=result['connected_count'],
            average_risk_score=result['avg_risk'] or 0.0,
            high_risk_entities=result['high_risk_entities'][:10],  # Top 10
            risk_paths=[],  # TODO: ê²½ë¡œ ë¶„ì„ ì¶”ê°€
            analysis_timestamp=datetime.now()
        )
        
    except HTTPException:
        # HTTPExceptionì€ ê·¸ëŒ€ë¡œ re-raise
        raise
    except Exception as e:
        logger.error(f"Error analyzing network risk: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/api/v1/graph/companies/{company_id}")
async def get_company_details(company_id: str):
    """ê¸°ì—… ìƒì„¸ ì •ë³´ ì¡°íšŒ"""
    try:
        # ì…ë ¥ ê²€ì¦
        if not company_id or not company_id.strip():
            raise HTTPException(status_code=400, detail="Company ID is required")
        
        query = """
        MATCH (c:Company {id: $company_id})
        OPTIONAL MATCH (c)-[r:MENTIONED_IN]->(n:NewsArticle)
        WITH c, count(n) as news_count, avg(r.sentiment) as avg_sentiment
        OPTIONAL MATCH (c)-[:AFFECTS]-(e:Event)
        RETURN c as company, 
               news_count,
               avg_sentiment,
               count(e) as event_count
        """
        
        logger.info(f"Querying company details for ID: {company_id}")
        results = neo4j_driver.execute_read(query, company_id=company_id)
        
        if not results:
            logger.info(f"Company not found: {company_id}")
            raise HTTPException(status_code=404, detail=f"Company with ID '{company_id}' not found")
        
        logger.info(f"Successfully retrieved company details for ID: {company_id}")
        return results[0]
        
    except HTTPException:
        # HTTPExceptionì€ ê·¸ëŒ€ë¡œ re-raise (404, 400 ë“±)
        raise
    except Exception as e:
        logger.error(f"Error getting company details for ID '{company_id}': {e}")
        # Neo4j ì—°ê²° ê´€ë ¨ ì˜¤ë¥˜ì¸ì§€ í™•ì¸
        if "connection" in str(e).lower() or "timeout" in str(e).lower():
            raise HTTPException(status_code=503, detail="Database connection unavailable")
        else:
            raise HTTPException(status_code=500, detail="Internal server error")

@app.get("/api/v1/graph/cache/stats")
async def get_cache_stats():
    """ì—”í‹°í‹° ìºì‹œ í†µê³„ ì¡°íšŒ"""
    try:
        return entity_cache.get_cache_stats()
    except Exception as e:
        logger.error(f"Error getting cache stats: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/api/v1/graph/cache/refresh")
async def refresh_cache():
    """ì—”í‹°í‹° ìºì‹œ ìˆ˜ë™ ìƒˆë¡œê³ ì¹¨"""
    try:
        # ê°•ì œ ìƒˆë¡œê³ ì¹¨
        entity_cache.get_companies(force_refresh=True)
        entity_cache.get_persons(force_refresh=True)
        
        stats = entity_cache.get_cache_stats()
        return {
            "message": "Cache refreshed successfully",
            "stats": stats
        }
    except Exception as e:
        logger.error(f"Error refreshing cache: {e}")
        raise HTTPException(status_code=500, detail=str(e))

# Week 4: ìµœì í™”ëœ ì¿¼ë¦¬ API
@app.get("/api/v1/graph/optimized/company/{company_id}")
async def get_optimized_company_info(company_id: str):
    """ìµœì í™”ëœ ê¸°ì—… ì •ë³´ ì¡°íšŒ (< 50ms ëª©í‘œ)"""
    try:
        optimized_queries = get_optimized_queries()
        company_info = optimized_queries.get_company_basic_info(company_id)
        
        if not company_info:
            raise HTTPException(status_code=404, detail=f"Company '{company_id}' not found")
        
        return {
            "company": company_info,
            "cached": True  # ìºì‹œ ì—¬ë¶€ëŠ” ì‹¤ì œë¡œëŠ” ë‚´ë¶€ ë¡œì§ì—ì„œ ê²°ì •ë¨
        }
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error in optimized company query: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/api/v1/graph/optimized/company/{company_id}/connections")
async def get_optimized_company_connections(company_id: str, limit: int = 10):
    """ìµœì í™”ëœ ê¸°ì—… ì—°ê²° ê´€ê³„ ì¡°íšŒ (< 50ms ëª©í‘œ)"""
    try:
        optimized_queries = get_optimized_queries()
        connections = optimized_queries.get_company_connections(company_id, limit)
        
        return {
            "company_id": company_id,
            "connections": connections,
            "total": len(connections)
        }
    except Exception as e:
        logger.error(f"Error in optimized connections query: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/api/v1/graph/optimized/company/{company_id}/risk-paths")
async def get_optimized_risk_paths(company_id: str, max_depth: int = 3):
    """ìµœì í™”ëœ ë¦¬ìŠ¤í¬ ì „íŒŒ ê²½ë¡œ ë¶„ì„ (< 200ms ëª©í‘œ)"""
    try:
        optimized_queries = get_optimized_queries()
        risk_paths = optimized_queries.analyze_risk_propagation_paths(company_id, max_depth)
        
        return {
            "company_id": company_id,
            "risk_paths": risk_paths,
            "analysis_depth": max_depth,
            "total_paths": len(risk_paths)
        }
    except Exception as e:
        logger.error(f"Error in optimized risk paths query: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/api/v1/graph/optimized/company/{company_id}/network-risk")
async def get_optimized_network_risk(company_id: str):
    """ìµœì í™”ëœ ë„¤íŠ¸ì›Œí¬ ë¦¬ìŠ¤í¬ ìš”ì•½ (< 100ms ëª©í‘œ)"""
    try:
        optimized_queries = get_optimized_queries()
        network_risk = optimized_queries.calculate_network_risk_summary(company_id)
        
        if not network_risk:
            raise HTTPException(status_code=404, detail=f"Company '{company_id}' not found")
        
        return network_risk
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error in optimized network risk query: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/api/v1/graph/optimized/sectors/risk-distribution")
async def get_optimized_sector_risk_distribution(limit: int = 20):
    """ìµœì í™”ëœ ì„¹í„°ë³„ ë¦¬ìŠ¤í¬ ë¶„í¬ (< 150ms ëª©í‘œ)"""
    try:
        optimized_queries = get_optimized_queries()
        distribution = optimized_queries.get_sector_risk_distribution(limit)
        
        return {
            "sectors": distribution,
            "total_sectors": len(distribution),
            "analysis_timestamp": datetime.now().isoformat()
        }
    except Exception as e:
        logger.error(f"Error in sector risk distribution query: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/api/v1/graph/optimized/events/recent")
async def get_optimized_recent_events(
    company_id: Optional[str] = None, 
    days: int = 30, 
    limit: int = 50
):
    """ìµœì í™”ëœ ìµœê·¼ ë¦¬ìŠ¤í¬ ì´ë²¤íŠ¸ ì¡°íšŒ (< 100ms ëª©í‘œ)"""
    try:
        optimized_queries = get_optimized_queries()
        events = optimized_queries.get_recent_risk_events(company_id, days, limit)
        
        return {
            "events": events,
            "filters": {
                "company_id": company_id,
                "days": days,
                "limit": limit
            },
            "total_events": len(events)
        }
    except Exception as e:
        logger.error(f"Error in recent events query: {e}")
        raise HTTPException(status_code=500, detail=str(e))

# Week 4: ì„±ëŠ¥ íŠœë‹ API
@app.get("/api/v1/graph/performance/stats")
async def get_performance_stats():
    """ì¿¼ë¦¬ ì„±ëŠ¥ í†µê³„ ì¡°íšŒ"""
    try:
        optimized_queries = get_optimized_queries()
        performance_tuner = get_performance_tuner()
        
        return {
            "optimized_queries": optimized_queries.get_performance_stats(),
            "query_analysis": performance_tuner.get_query_performance_summary(),
            "slow_queries": performance_tuner.get_slow_queries(10)
        }
    except Exception as e:
        logger.error(f"Error getting performance stats: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/api/v1/graph/performance/indexes")
async def get_index_status():
    """ì¸ë±ìŠ¤ ìƒíƒœ ë° ë¶„ì„"""
    try:
        performance_tuner = get_performance_tuner()
        index_analysis = performance_tuner.analyze_index_usage()
        
        return index_analysis
    except Exception as e:
        logger.error(f"Error getting index status: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/api/v1/graph/performance/indexes/create")
async def create_performance_indexes():
    """ì„±ëŠ¥ ìµœì í™” ì¸ë±ìŠ¤ ìƒì„±"""
    try:
        performance_tuner = get_performance_tuner()
        results = performance_tuner.create_performance_indexes()
        
        return {
            "message": "Performance indexes creation completed",
            "results": results,
            "success_count": sum(1 for success in results.values() if success),
            "total_count": len(results)
        }
    except Exception as e:
        logger.error(f"Error creating performance indexes: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/api/v1/graph/performance/diagnostics")
async def run_performance_diagnostics():
    """ì„±ëŠ¥ ì§„ë‹¨ ì‹¤í–‰"""
    try:
        performance_tuner = get_performance_tuner()
        diagnostics = performance_tuner.run_performance_diagnostics()
        
        return diagnostics
    except Exception as e:
        logger.error(f"Error running performance diagnostics: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/api/v1/graph/performance/cache/warmup")
async def warmup_query_cache(company_ids: List[str]):
    """ì¿¼ë¦¬ ìºì‹œ ì›Œë°ì—…"""
    try:
        optimized_queries = get_optimized_queries()
        optimized_queries.warm_up_cache(company_ids)
        
        return {
            "message": "Cache warmup completed",
            "warmed_companies": len(company_ids),
            "cache_stats": optimized_queries.get_performance_stats()
        }
    except Exception as e:
        logger.error(f"Error warming up cache: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/api/v1/graph/performance/query/analyze")
async def analyze_query_performance(
    query: str,
    parameters: Optional[Dict[str, Any]] = None
):
    """ì¿¼ë¦¬ ì„±ëŠ¥ ë¶„ì„"""
    try:
        performance_tuner = get_performance_tuner()
        
        # ì¿¼ë¦¬ í”Œëœ ë¶„ì„
        query_plan = performance_tuner.analyze_query_plan(query, parameters or {})
        
        # ìµœì í™” ì œì•ˆ
        optimization = performance_tuner.optimize_query(query)
        
        return {
            "query_plan": {
                "execution_time_ms": query_plan.execution_time_ms,
                "db_hits": query_plan.db_hits,
                "actual_rows": query_plan.actual_rows,
                "index_usage": query_plan.index_usage
            },
            "optimization": optimization,
            "performance_classification": (
                "excellent" if query_plan.execution_time_ms < 50 else
                "good" if query_plan.execution_time_ms < 200 else
                "needs_optimization"
            )
        }
    except Exception as e:
        logger.error(f"Error analyzing query performance: {e}")
        raise HTTPException(status_code=500, detail=str(e))

if __name__ == "__main__":
    import uvicorn
    port = int(os.getenv("SERVICE_PORT", "8003"))
    uvicorn.run(app, host="0.0.0.0", port=port)