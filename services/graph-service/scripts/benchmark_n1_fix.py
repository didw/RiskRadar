#!/usr/bin/env python3
"""N+1 ë¬¸ì œ í•´ê²° ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬"""
import sys
import os
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

import time
import statistics
from typing import List, Dict
from unittest.mock import patch, MagicMock

from src.kafka.entity_cache import entity_cache
from src.algorithms.batch_entity_matching import batch_matcher

def generate_test_entities(count: int) -> List[Dict]:
    """í…ŒìŠ¤íŠ¸ ì—”í‹°í‹° ìƒì„±"""
    entities = []
    
    # 50% ê¸°ì—…, 30% ì¸ë¬¼, 20% ê¸°íƒ€
    company_count = int(count * 0.5)
    person_count = int(count * 0.3)
    other_count = count - company_count - person_count
    
    for i in range(company_count):
        entities.append({
            'type': 'COMPANY',
            'text': f'Test Company {i}',
            'confidence': 0.9
        })
    
    for i in range(person_count):
        entities.append({
            'type': 'PERSON',
            'text': f'Test Person {i}',
            'confidence': 0.8
        })
    
    for i in range(other_count):
        entities.append({
            'type': 'EVENT',
            'text': f'Test Event {i}',
            'confidence': 0.7
        })
    
    return entities

def simulate_old_approach(entities: List[Dict]) -> Dict:
    """ê¸°ì¡´ ë°©ì‹ ì‹œë®¬ë ˆì´ì…˜ (N+1 ë¬¸ì œ)"""
    query_count = 0
    total_time = 0
    
    def mock_db_query(*args, **kwargs):
        nonlocal query_count
        query_count += 1
        time.sleep(0.002)  # 2ms DB ì¿¼ë¦¬ ì§€ì—°
        return []
    
    start_time = time.time()
    
    # ê° ì—”í‹°í‹°ë§ˆë‹¤ ì „ì²´ í…Œì´ë¸” ìŠ¤ìº”
    for entity in entities:
        if entity['type'] == 'COMPANY':
            mock_db_query("MATCH (c:Company) RETURN c")
        elif entity['type'] == 'PERSON':
            mock_db_query("MATCH (p:Person) RETURN p")
    
    total_time = time.time() - start_time
    
    return {
        'query_count': query_count,
        'total_time': total_time,
        'avg_time_per_entity': total_time / len(entities) if entities else 0
    }

def benchmark_batch_approach(entities: List[Dict]) -> Dict:
    """ë°°ì¹˜ ë°©ì‹ ë²¤ì¹˜ë§ˆí¬"""
    query_count = 0
    
    def mock_cache_companies():
        nonlocal query_count
        query_count += 1
        time.sleep(0.01)  # 10ms ìºì‹œ ì¡°íšŒ
        return [{'id': f'company-{i}', 'name': f'Company {i}', 'aliases': []} for i in range(1000)]
    
    def mock_cache_persons():
        nonlocal query_count
        query_count += 1
        time.sleep(0.008)  # 8ms ìºì‹œ ì¡°íšŒ
        return [{'id': f'person-{i}', 'name': f'Person {i}', 'aliases': []} for i in range(500)]
    
    start_time = time.time()
    
    with patch.object(entity_cache, 'get_companies', side_effect=mock_cache_companies), \
         patch.object(entity_cache, 'get_persons', side_effect=mock_cache_persons):
        
        results = batch_matcher.match_entities_batch(entities)
    
    total_time = time.time() - start_time
    
    return {
        'query_count': query_count,
        'total_time': total_time,
        'avg_time_per_entity': total_time / len(entities) if entities else 0,
        'match_count': len(results)
    }

def run_benchmark():
    """ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰"""
    print("ğŸš€ N+1 ë¬¸ì œ í•´ê²° ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬")
    print("=" * 50)
    
    test_cases = [10, 50, 100, 200, 500]
    
    results = []
    
    for entity_count in test_cases:
        print(f"\nğŸ“Š ì—”í‹°í‹° ìˆ˜: {entity_count}ê°œ")
        print("-" * 30)
        
        entities = generate_test_entities(entity_count)
        
        # ê¸°ì¡´ ë°©ì‹ (N+1 ë¬¸ì œ)
        old_result = simulate_old_approach(entities)
        
        # ê°œì„ ëœ ë°©ì‹ (ë°°ì¹˜ ë§¤ì¹­)
        new_result = benchmark_batch_approach(entities)
        
        # ì„±ëŠ¥ í–¥ìƒ ê³„ì‚°
        time_improvement = old_result['total_time'] / new_result['total_time']
        query_reduction = old_result['query_count'] / new_result['query_count']
        
        print(f"ğŸš¨ ê¸°ì¡´ ë°©ì‹:")
        print(f"   ì¿¼ë¦¬ ìˆ˜: {old_result['query_count']}ë²ˆ")
        print(f"   ì´ ì‹œê°„: {old_result['total_time']:.3f}ì´ˆ")
        print(f"   í‰ê· : {old_result['avg_time_per_entity']:.4f}ì´ˆ/ì—”í‹°í‹°")
        
        print(f"âœ… ê°œì„ ëœ ë°©ì‹:")
        print(f"   ì¿¼ë¦¬ ìˆ˜: {new_result['query_count']}ë²ˆ")
        print(f"   ì´ ì‹œê°„: {new_result['total_time']:.3f}ì´ˆ")
        print(f"   í‰ê· : {new_result['avg_time_per_entity']:.4f}ì´ˆ/ì—”í‹°í‹°")
        print(f"   ë§¤ì¹­: {new_result['match_count']}ê°œ")
        
        print(f"ğŸ“ˆ ì„±ëŠ¥ í–¥ìƒ:")
        print(f"   ì‹œê°„ ë‹¨ì¶•: {time_improvement:.1f}ë°°")
        print(f"   ì¿¼ë¦¬ ê°ì†Œ: {query_reduction:.1f}ë°°")
        
        results.append({
            'entity_count': entity_count,
            'old_time': old_result['total_time'],
            'new_time': new_result['total_time'],
            'time_improvement': time_improvement,
            'query_reduction': query_reduction
        })
    
    # ì „ì²´ ìš”ì•½
    print("\n" + "=" * 50)
    print("ğŸ“‹ ë²¤ì¹˜ë§ˆí¬ ìš”ì•½")
    print("=" * 50)
    
    avg_time_improvement = statistics.mean([r['time_improvement'] for r in results])
    avg_query_reduction = statistics.mean([r['query_reduction'] for r in results])
    
    print(f"í‰ê·  ì‹œê°„ ë‹¨ì¶•: {avg_time_improvement:.1f}ë°°")
    print(f"í‰ê·  ì¿¼ë¦¬ ê°ì†Œ: {avg_query_reduction:.1f}ë°°")
    
    # í™•ì¥ì„± ë¶„ì„
    print(f"\nğŸ” í™•ì¥ì„± ë¶„ì„:")
    print(f"ì—”í‹°í‹° 10ê°œ â†’ 500ê°œ (50ë°° ì¦ê°€)")
    result_10 = next(r for r in results if r['entity_count'] == 10)
    result_500 = next(r for r in results if r['entity_count'] == 500)
    
    old_scaling = result_500['old_time'] / result_10['old_time']
    new_scaling = result_500['new_time'] / result_10['new_time']
    
    print(f"ê¸°ì¡´ ë°©ì‹: {old_scaling:.1f}ë°° ì¦ê°€ (ì„ í˜• ì¦ê°€)")
    print(f"ê°œì„ ëœ ë°©ì‹: {new_scaling:.1f}ë°° ì¦ê°€ (ìƒìˆ˜ ì‹œê°„)")
    
    # ì‹¤ì œ ìš´ì˜ í™˜ê²½ ì˜ˆì¸¡
    print(f"\nğŸ­ ì‹¤ì œ ìš´ì˜ í™˜ê²½ ì˜ˆì¸¡:")
    print(f"ë‰´ìŠ¤ 1ê±´ë‹¹ í‰ê·  ì—”í‹°í‹° 15ê°œ ê¸°ì¤€")
    print(f"ì¼ì¼ ë‰´ìŠ¤ 1,000ê±´ ì²˜ë¦¬ ì‹œ:")
    
    daily_entities = 1000 * 15
    result_baseline = next(r for r in results if r['entity_count'] >= 50)  # ê°€ì¥ ê°€ê¹Œìš´ ê°’
    
    # ì„ í˜• ìŠ¤ì¼€ì¼ë§ìœ¼ë¡œ ì˜ˆì¸¡
    scale_factor = daily_entities / result_baseline['entity_count']
    
    old_daily_time = result_baseline['old_time'] * scale_factor
    new_daily_time = result_baseline['new_time'] * scale_factor
    
    print(f"ê¸°ì¡´ ë°©ì‹: {old_daily_time:.1f}ì´ˆ ({old_daily_time/60:.1f}ë¶„)")
    print(f"ê°œì„ ëœ ë°©ì‹: {new_daily_time:.1f}ì´ˆ ({new_daily_time/60:.1f}ë¶„)")
    print(f"ì¼ì¼ ì ˆì•½ ì‹œê°„: {(old_daily_time - new_daily_time)/60:.1f}ë¶„")

if __name__ == "__main__":
    run_benchmark()