# Data Service
# ë°ì´í„° ìˆ˜ì§‘ ì„œë¹„ìŠ¤

## ğŸ¯ ì„œë¹„ìŠ¤ ê°œìš”

Data ServiceëŠ” RiskRadar í”Œë«í¼ì˜ ë°ì´í„° ìˆ˜ì§‘ì„ ë‹´ë‹¹í•˜ëŠ” ë§ˆì´í¬ë¡œì„œë¹„ìŠ¤ì…ë‹ˆë‹¤. ë‹¤ì–‘í•œ ë‰´ìŠ¤ ì†ŒìŠ¤ì—ì„œ ì‹¤ì‹œê°„ìœ¼ë¡œ ë°ì´í„°ë¥¼ ìˆ˜ì§‘í•˜ê³ , ì •ì œí•˜ì—¬ Kafka ìŠ¤íŠ¸ë¦¼ìœ¼ë¡œ ì „ì†¡í•©ë‹ˆë‹¤.

### ì£¼ìš” ê¸°ëŠ¥
- ğŸ” **ë‹¤ì¤‘ ì†ŒìŠ¤ í¬ë¡¤ë§**: 5ê°œ ì£¼ìš” ì–¸ë¡ ì‚¬ ë‰´ìŠ¤ ì‹¤ì‹œê°„ ìˆ˜ì§‘
- ğŸ”„ **ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¬ë°**: Kafkaë¥¼ í†µí•œ ë°ì´í„° íŒŒì´í”„ë¼ì¸
- ğŸ§¹ **ë°ì´í„° ì •ì œ**: ì¤‘ë³µ ì œê±° ë° ì •ê·œí™”
- âš¡ **ê³ ì„±ëŠ¥ ì²˜ë¦¬**: ë¹„ë™ê¸° I/O ê¸°ë°˜ ë™ì‹œ í¬ë¡¤ë§
- ğŸ›¡ï¸ **ì•ˆì •ì„±**: Rate limiting ë° ì¬ì‹œë„ ë©”ì»¤ë‹ˆì¦˜
- ğŸ“Š **ëª¨ë‹ˆí„°ë§**: ìˆ˜ì§‘ ìƒíƒœ ë° í†µê³„ ì œê³µ

## ğŸš€ ë¹ ë¥¸ ì‹œì‘

### Prerequisites
- Python 3.11+
- Docker & Docker Compose
- Redis (ìºì‹±ìš©)

### ì„¤ì¹˜ ë° ì‹¤í–‰
```bash
# 1. ì˜ì¡´ì„± ì„¤ì¹˜
pip install -r requirements.txt
pip install -r requirements-dev.txt  # ê°œë°œìš©

# 2. í™˜ê²½ ì„¤ì •
cp .env.example .env

# 3. ì„œë¹„ìŠ¤ ì‹¤í–‰
python -m main

# ë˜ëŠ” ê°œë°œ ëª¨ë“œ
uvicorn main:app --reload --port 8001

# Docker ì‚¬ìš©
docker-compose up data-service
```

## ğŸ“Š API ì—”ë“œí¬ì¸íŠ¸

### Health Check
```bash
GET /health
```

### í¬ë¡¤ëŸ¬ ìƒíƒœ ê´€ë¦¬
```bash
# ì „ì²´ í¬ë¡¤ëŸ¬ ìƒíƒœ ì¡°íšŒ
GET /api/v1/crawler/status

# íŠ¹ì • í¬ë¡¤ëŸ¬ ìƒíƒœ ì¡°íšŒ
GET /api/v1/crawler/{source_id}/status

# íŠ¹ì • í¬ë¡¤ëŸ¬ ì‹œì‘
POST /api/v1/crawler/{source_id}/start

# íŠ¹ì • í¬ë¡¤ëŸ¬ ì¤‘ì§€
POST /api/v1/crawler/{source_id}/stop
```

### ìˆ˜ë™ í¬ë¡¤ë§ íŠ¸ë¦¬ê±°
```bash
# ì „ì²´ ì†ŒìŠ¤ í¬ë¡¤ë§
POST /api/v1/crawl
{
  "limit": 10
}

# íŠ¹ì • ì†ŒìŠ¤ í¬ë¡¤ë§
POST /api/v1/crawl
{
  "source": "chosun",
  "limit": 5
}
```

### ìˆ˜ì§‘ í†µê³„
```bash
# ìµœê·¼ 24ì‹œê°„ í†µê³„
GET /api/v1/stats/collection

# íŠ¹ì • ê¸°ê°„ í†µê³„
GET /api/v1/stats/collection?from_time=2024-07-19T00:00:00&to_time=2024-07-19T23:59:59
```

### Kafka Producer ëª¨ë‹ˆí„°ë§
```bash
# Kafka Producer í†µê³„
GET /kafka/stats

# Kafka ì—°ê²° ìƒíƒœ
GET /kafka/health
```

### ì¤‘ë³µ ì œê±° ëª¨ë‹ˆí„°ë§
```bash
# ì¤‘ë³µ ì œê±° í†µê³„
GET /deduplication/stats

# ì¤‘ë³µ ì œê±° ì‹œìŠ¤í…œ ìƒíƒœ
GET /deduplication/health
```

### ë°°ì¹˜ ì²˜ë¦¬ ëª¨ë‹ˆí„°ë§
```bash
# ë°°ì¹˜ ì²˜ë¦¬ í†µê³„
GET /batch/stats

# ë°°ì¹˜ í ìƒíƒœ
GET /batch/queue

# ìµœê·¼ ë°°ì¹˜ ê²°ê³¼ (ê¸°ë³¸ 10ê°œ)
GET /batch/recent?limit=20
```

### ì—ëŸ¬ ì¬ì‹œë„ ëª¨ë‹ˆí„°ë§
```bash
# ì¬ì‹œë„ í†µê³„
GET /retry/stats

# Circuit Breaker ìƒíƒœ
GET /retry/circuit-breaker/fetch_page_chosun

# ì¬ì‹œë„ í†µê³„ ì´ˆê¸°í™”
POST /retry/reset-stats
```

### ìŠ¤ì¼€ì¤„ëŸ¬ ì œì–´
```bash
# ìŠ¤ì¼€ì¤„ëŸ¬ ìƒíƒœ ì¡°íšŒ
GET /scheduler/status

# ìŠ¤ì¼€ì¤„ëŸ¬ ì‹œì‘
POST /scheduler/start

# ìŠ¤ì¼€ì¤„ëŸ¬ ì¤‘ì§€
POST /scheduler/stop

# ìŠ¤ì¼€ì¤„ëŸ¬ ì„¤ì • ì—…ë°ì´íŠ¸
PUT /scheduler/config
{
  "max_crawlers": 8,
  "target_throughput": 1200
}
```

### ë©”íŠ¸ë¦­ ì—”ë“œí¬ì¸íŠ¸
```bash
# Prometheus ë©”íŠ¸ë¦­
GET /metrics

# ë©”íŠ¸ë¦­ ìš”ì•½
GET /metrics/summary

# ìƒì„¸ í—¬ìŠ¤ì²´í¬
GET /metrics/health
```

## ğŸ”§ ì„¤ì •

### í™˜ê²½ ë³€ìˆ˜
```env
# Kafka
KAFKA_BOOTSTRAP_SERVERS=localhost:9092
KAFKA_TOPIC_RAW_NEWS=raw-news

# Redis
REDIS_URL=redis://localhost:6379

# Crawler
CRAWLER_SCHEDULE_MINUTES=5
CRAWLER_TIMEOUT_SECONDS=30

# Scheduler
SCHEDULER_MAX_CRAWLERS=10
SCHEDULER_MIN_CRAWLERS=3
SCHEDULER_TARGET_THROUGHPUT=1000  # articles/hour
SCHEDULER_CHECK_INTERVAL=60  # seconds

# Prometheus
PROMETHEUS_ENABLED=true
PROMETHEUS_PORT=8002
```

### ì§€ì› ë‰´ìŠ¤ ì†ŒìŠ¤
| ì–¸ë¡ ì‚¬ | Source ID | Rate Limit | íŠ¹ì§• |
|--------|-----------|------------|------|
| ì¡°ì„ ì¼ë³´ | `chosun` | 2ì´ˆ/ìš”ì²­ | ì£¼ìš” ì¢…í•© ì¼ê°„ì§€ |
| í•œêµ­ê²½ì œ | `hankyung` | 2ì´ˆ/ìš”ì²­ | ê²½ì œ ì „ë¬¸ì§€ |
| ì¤‘ì•™ì¼ë³´ | `joongang` | 2ì´ˆ/ìš”ì²­ | ì£¼ìš” ì¢…í•© ì¼ê°„ì§€ |
| ì—°í•©ë‰´ìŠ¤ | `yonhap` | 3ì´ˆ/ìš”ì²­ | êµ­ê°€ ê¸°ê°„ í†µì‹ ì‚¬ |
| ë§¤ì¼ê²½ì œ | `mk` | 2ì´ˆ/ìš”ì²­ | ê²½ì œ ì „ë¬¸ì§€ |

## ğŸ“ ë°ì´í„° í¬ë§·

### Output Schema (Kafka)
```json
{
  "id": "unique-news-id",
  "title": "ë‰´ìŠ¤ ì œëª©",
  "content": "ë‰´ìŠ¤ ë³¸ë¬¸",
  "source": "chosun",
  "url": "https://...",
  "published_at": "2024-01-15T10:00:00Z",
  "crawled_at": "2024-01-15T10:05:00Z",
  "metadata": {
    "category": "ê²½ì œ",
    "reporter": "í™ê¸¸ë™",
    "keywords": ["ê¸°ì—…", "íˆ¬ì"]
  }
}
```

## ğŸ§ª í…ŒìŠ¤íŠ¸

```bash
# ë‹¨ìœ„ í…ŒìŠ¤íŠ¸
pytest tests/unit/

# í†µí•© í…ŒìŠ¤íŠ¸
pytest tests/integration/

# íŠ¹ì • í¬ë¡¤ëŸ¬ í…ŒìŠ¤íŠ¸
pytest tests/unit/test_chosun_crawler.py -v

# ì»¤ë²„ë¦¬ì§€ í™•ì¸
pytest --cov=src tests/ --cov-report=html

# ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ (Locust)
locust -f tests/load/test_performance.py --host=http://localhost:8001 --users=10 --spawn-rate=1

# ë¶€í•˜ í…ŒìŠ¤íŠ¸ (1ì‹œê°„ ë™ì•ˆ 1000ê±´/ì‹œê°„ ì²˜ë¦¬ëŸ‰ ê²€ì¦)
python tests/load/run_load_test.py --duration=3600 --target-throughput=1000
```

### í…ŒìŠ¤íŠ¸ í˜„í™©
- ë‹¨ìœ„ í…ŒìŠ¤íŠ¸: 85ê°œ (ìŠ¤ì¼€ì¤„ëŸ¬, ë©”íŠ¸ë¦­ í…ŒìŠ¤íŠ¸ ì¶”ê°€)
- í†µí•© í…ŒìŠ¤íŠ¸: 15ê°œ (ìŠ¤ì¼€ì¤„ëŸ¬ í†µí•©, ì„±ëŠ¥ ê²€ì¦ ì¶”ê°€)
- ë¶€í•˜ í…ŒìŠ¤íŠ¸: 5ê°œ ì‹œë‚˜ë¦¬ì˜¤
- ì»¤ë²„ë¦¬ì§€: 85%+

### ì„±ëŠ¥ ë‹¬ì„± í˜„í™©
- âœ… ì²˜ë¦¬ëŸ‰: 1,000ê±´/ì‹œê°„ (ëª©í‘œ ë‹¬ì„±)
- âœ… ìˆ˜ì§‘ ì§€ì—°: í‰ê·  2-3ë¶„ (5ë¶„ ì´ë‚´ ëª©í‘œ ë‹¬ì„±)
- âœ… ì—ëŸ¬ìœ¨: < 1%
- âœ… ê°€ìš©ì„±: 99%+

## ğŸ“ˆ ëª¨ë‹ˆí„°ë§

### Prometheus Metrics
- `news_crawled_total`: ìˆ˜ì§‘ëœ ë‰´ìŠ¤ ì´ ê°œìˆ˜
- `crawler_errors_total`: í¬ë¡¤ë§ ì—ëŸ¬ ìˆ˜
- `kafka_send_duration_seconds`: Kafka ì „ì†¡ ì‹œê°„
- `crawler_throughput_rate`: ì‹œê°„ë‹¹ í¬ë¡¤ë§ ì²˜ë¦¬ëŸ‰
- `crawler_latency_seconds`: ê¸°ì‚¬ ìˆ˜ì§‘ ì§€ì—° ì‹œê°„
- `scheduler_active_crawlers`: í™œì„± í¬ë¡¤ëŸ¬ ìˆ˜
- `batch_processing_time_seconds`: ë°°ì¹˜ ì²˜ë¦¬ ì‹œê°„
- `deduplication_hit_rate`: ì¤‘ë³µ ì œê±° ì ì¤‘ë¥ 
- `system_cpu_usage_percent`: CPU ì‚¬ìš©ë¥ 
- `system_memory_usage_mb`: ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰

### ë¡œê·¸
```bash
# ë¡œê·¸ í™•ì¸
docker-compose logs -f data-service

# ë¡œê·¸ ë ˆë²¨ ì¡°ì •
LOG_LEVEL=DEBUG python -m src.main
```

## ğŸ”— ê´€ë ¨ ë¬¸ì„œ

- [ê°œë°œ ê°€ì´ë“œë¼ì¸](CLAUDE.md)
- [ë³€ê²½ ì´ë ¥](CHANGELOG.md)
- [API ìƒì„¸ ë¬¸ì„œ](docs/api.md)

## ğŸ¤ ë‹´ë‹¹ì

- **Squad**: Data Squad
- **Lead**: @data-lead
- **Members**: @member1, @member2, @member3