# Data Service
# ë°ì´í„° ìˆ˜ì§‘ ì„œë¹„ìŠ¤

## ğŸ¯ ì„œë¹„ìŠ¤ ê°œìš”

Data ServiceëŠ” RiskRadar í”Œë«í¼ì˜ ë°ì´í„° ìˆ˜ì§‘ì„ ë‹´ë‹¹í•˜ëŠ” ë§ˆì´í¬ë¡œì„œë¹„ìŠ¤ì…ë‹ˆë‹¤. ë‹¤ì–‘í•œ ë‰´ìŠ¤ ì†ŒìŠ¤ì—ì„œ ì‹¤ì‹œê°„ìœ¼ë¡œ ë°ì´í„°ë¥¼ ìˆ˜ì§‘í•˜ê³ , ì •ì œí•˜ì—¬ Kafka ìŠ¤íŠ¸ë¦¼ìœ¼ë¡œ ì „ì†¡í•©ë‹ˆë‹¤.

### ì£¼ìš” ê¸°ëŠ¥
- ğŸ” **ë‹¤ì¤‘ ì†ŒìŠ¤ í¬ë¡¤ë§**: 7ê°œ ë‰´ìŠ¤ ì†ŒìŠ¤ ì‹¤ì‹œê°„ ìˆ˜ì§‘ (1,000ê±´/ì‹œê°„)
- ğŸŒ **ë‹¤ì¤‘ í”„ë¡œí† ì½œ ì§€ì›**: ì›¹ ìŠ¤í¬ë˜í•‘ + RSS í”¼ë“œ íŒŒì‹± (Week 3 ì¶”ê°€)
- ğŸ“¡ **êµ­ì œ ë‰´ìŠ¤ ìˆ˜ì§‘**: Guardian, BBC RSS í¬ë¡¤ëŸ¬ ì¶”ê°€ (Week 3 ì¶”ê°€)
- ğŸ”„ **ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¬ë°**: Kafka ë°°ì¹˜ ì²˜ë¦¬ ë° ì••ì¶• ì „ì†¡
- ğŸ§¹ **ì§€ëŠ¥í˜• ì¤‘ë³µì œê±°**: Bloom Filter + Jaccard ìœ ì‚¬ë„ ê¸°ë°˜
- âš¡ **ê³ ì„±ëŠ¥ ì•„í‚¤í…ì²˜**: ë¹„ë™ê¸° ë³‘ë ¬ ì²˜ë¦¬ (20ê°œ ê¸°ì‚¬ ë™ì‹œ)
- ğŸ›¡ï¸ **ì¥ì•  ëŒ€ì‘**: Circuit Breaker + ì§€ìˆ˜ ë°±ì˜¤í”„
- ğŸ“Š **Prometheus ëª¨ë‹ˆí„°ë§**: ì‹¤ì‹œê°„ ì„±ëŠ¥ ë©”íŠ¸ë¦­

## ğŸš€ ë¹ ë¥¸ ì‹œì‘

### Prerequisites
- Python 3.11+
- Docker & Docker Compose
- Redis (ìºì‹±ìš©)
- Kafka (ë©”ì‹œì§€ í)

### ì„¤ì¹˜ ë° ì‹¤í–‰
```bash
# 1. ì˜ì¡´ì„± ì„¤ì¹˜
pip install -r requirements.txt
pip install -r requirements-dev.txt  # ê°œë°œìš©

# 2. í™˜ê²½ ì„¤ì •
cp .env.example .env

# 3. ì„œë¹„ìŠ¤ ì‹¤í–‰
python -m src.main

# ë˜ëŠ” ê°œë°œ ëª¨ë“œ
uvicorn src.main:app --reload --port 8001

# Docker ì‚¬ìš©
docker-compose up data-service
```

## ğŸ“Š API ì—”ë“œí¬ì¸íŠ¸

### Health Check
```bash
GET /health
```

### ìŠ¤ì¼€ì¤„ëŸ¬ ì œì–´
```bash
# ìŠ¤ì¼€ì¤„ëŸ¬ ì‹œì‘/ì¤‘ì§€
POST /api/v1/scheduler/start
POST /api/v1/scheduler/stop

# ìŠ¤ì¼€ì¤„ëŸ¬ ìƒíƒœ ì¡°íšŒ
GET /api/v1/scheduler/stats
GET /api/v1/scheduler/tasks

# ìŠ¤ì¼€ì¤„ëŸ¬ ì„¤ì • ì—…ë°ì´íŠ¸
PUT /api/v1/scheduler/config
{
  "max_concurrent_crawlers": 5,
  "target_throughput": 1000
}
```

### ìˆ˜ë™ í¬ë¡¤ë§ íŠ¸ë¦¬ê±°
```bash
# ì „ì²´ ì†ŒìŠ¤ í¬ë¡¤ë§
POST /api/v1/crawl
{
  "limit": 10
}

# íŠ¹ì • ì†ŒìŠ¤ í¬ë¡¤ë§
POST /api/v1/crawl
{
  "sources": ["chosun", "yonhap", "guardian", "bbc"],
  "limit": 5,
  "priority": "high"
}
```

### ë©”íŠ¸ë¦­ ë° í†µê³„
```bash
# Prometheus ë©”íŠ¸ë¦­
GET /metrics

# ë©”íŠ¸ë¦­ í†µê³„ ìš”ì•½
GET /api/v1/metrics/stats

# íŠ¹ì • ê¸°ê°„ í†µê³„
GET /api/v1/stats/collection?from_time=2024-07-19T00:00:00&to_time=2024-07-19T23:59:59
```

### ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ (Prometheus)
```bash
# ìµœì í™”ëœ Kafka Producer í†µê³„
data_service_kafka_messages_sent_total
data_service_kafka_send_duration_seconds

# Bloom Filter ì¤‘ë³µ ì œê±° ë©”íŠ¸ë¦­
data_service_deduplication_rate
data_service_duplicate_articles_detected_total

# ë°°ì¹˜ ì²˜ë¦¬ ë©”íŠ¸ë¦­
data_service_batch_processing_time_seconds
data_service_batch_queue_size

# Circuit Breaker ìƒíƒœ
data_service_circuit_breaker_state{operation="fetch_page"}
data_service_retry_attempts_total{error_type="network"}
```

## ğŸ”§ ì„¤ì •

### í™˜ê²½ ë³€ìˆ˜
```env
# Kafka (ìµœì í™”ëœ ì„¤ì •)
KAFKA_BOOTSTRAP_SERVERS=localhost:9092
KAFKA_TOPIC_RAW_NEWS=raw-news
KAFKA_BATCH_SIZE=16384
KAFKA_LINGER_MS=10
KAFKA_COMPRESSION_TYPE=gzip

# Redis (Bloom Filter ì €ì¥)
REDIS_URL=redis://localhost:6379
REDIS_BLOOM_KEY=news_bloom_filter

# ê³ ì„±ëŠ¥ ìŠ¤ì¼€ì¤„ëŸ¬
SCHEDULER_MAX_CRAWLERS=5
SCHEDULER_MAX_ARTICLES=20
SCHEDULER_TARGET_THROUGHPUT=1000

# ë°°ì¹˜ ì²˜ë¦¬
BATCH_SIZE=100
BATCH_MAX_CONCURRENT=3
BATCH_FLUSH_INTERVAL=30

# ì¤‘ë³µ ì œê±°
DEDUP_BLOOM_CAPACITY=1000000
DEDUP_BLOOM_ERROR_RATE=0.001
DEDUP_SIMILARITY_THRESHOLD=0.85

# Prometheus
PROMETHEUS_ENABLED=true
METRICS_PORT=8002
```

### ì§€ì› ë‰´ìŠ¤ ì†ŒìŠ¤ (Week 3 í™•ì¥: 5ê°œ â†’ 7ê°œ)
| ì–¸ë¡ ì‚¬ | Source ID | ìˆ˜ì§‘ ê°„ê²© | ìš°ì„ ìˆœìœ„ | íŠ¹ì§• | í”„ë¡œí† ì½œ |
|--------|-----------|------------|----------|------|---------|
| ì—°í•©ë‰´ìŠ¤ | `yonhap` | 1ë¶„ | URGENT | ì†ë³´ ëŒ€ì‘ | ì›¹ ìŠ¤í¬ë˜í•‘ |
| ì¡°ì„ ì¼ë³´ | `chosun` | 3ë¶„ | HIGH | ì£¼ìš” ì¢…í•© ì¼ê°„ì§€ | ì›¹ ìŠ¤í¬ë˜í•‘ |
| í•œêµ­ê²½ì œ | `hankyung` | 3ë¶„ | HIGH | ê²½ì œ ì „ë¬¸ì§€ | ì›¹ ìŠ¤í¬ë˜í•‘ |
| ì¤‘ì•™ì¼ë³´ | `joongang` | 5ë¶„ | NORMAL | ì£¼ìš” ì¢…í•© ì¼ê°„ì§€ | ì›¹ ìŠ¤í¬ë˜í•‘ |
| ë§¤ì¼ê²½ì œ | `mk` | 5ë¶„ | NORMAL | ê²½ì œ ì „ë¬¸ì§€ | ì›¹ ìŠ¤í¬ë˜í•‘ |
| **Guardian** | `guardian` | 10ë¶„ | NORMAL | êµ­ì œ ë¹„ì¦ˆë‹ˆìŠ¤ ë‰´ìŠ¤ | **RSS í”¼ë“œ** â­ |
| **BBC** | `bbc` | 10ë¶„ | NORMAL | ê¸€ë¡œë²Œ ê²½ì œ ë‰´ìŠ¤ | **RSS í”¼ë“œ** â­ |

## ğŸ“ ë°ì´í„° í¬ë§·

### Output Schema (Kafka)
```json
{
  "id": "unique-news-id",
  "title": "ë‰´ìŠ¤ ì œëª©",
  "content": "ë‰´ìŠ¤ ë³¸ë¬¸",
  "source": "chosun",
  "url": "https://...",
  "published_at": "2024-01-15T10:00:00Z",
  "crawled_at": "2024-01-15T10:05:00Z",
  "metadata": {
    "category": "ê²½ì œ",
    "reporter": "í™ê¸¸ë™",
    "keywords": ["ê¸°ì—…", "íˆ¬ì"]
  }
}
```

## ğŸ§ª í…ŒìŠ¤íŠ¸

```bash
# ë‹¨ìœ„ í…ŒìŠ¤íŠ¸
pytest tests/unit/

# í†µí•© í…ŒìŠ¤íŠ¸
pytest tests/integration/

# íŠ¹ì • í¬ë¡¤ëŸ¬ í…ŒìŠ¤íŠ¸
pytest tests/unit/test_chosun_crawler.py -v

# ì»¤ë²„ë¦¬ì§€ í™•ì¸
pytest --cov=src tests/ --cov-report=html

# ë¶€í•˜ í…ŒìŠ¤íŠ¸
pytest tests/load/ -v

# ì„±ëŠ¥ ê²€ì¦ (1,000ê±´/ì‹œê°„ ì²˜ë¦¬ëŸ‰)
python tests/load/test_load_testing.py::TestLoadTesting::test_sustained_load
```

### í…ŒìŠ¤íŠ¸ í˜„í™© (Sprint 1 ì™„ë£Œ)
- âœ… **ë‹¨ìœ„ í…ŒìŠ¤íŠ¸**: 85ê°œ (ëª¨ë“  ì»´í¬ë„ŒíŠ¸ ì»¤ë²„)
- âœ… **í†µí•© í…ŒìŠ¤íŠ¸**: 15ê°œ (E2E, ìŠ¤ì¼€ì¤„ëŸ¬, ë©”íŠ¸ë¦­)
- âœ… **ë¶€í•˜ í…ŒìŠ¤íŠ¸**: 5ê°œ ì‹œë‚˜ë¦¬ì˜¤ (1,000ê±´/ì‹œê°„ ê²€ì¦)
- âœ… **ì»¤ë²„ë¦¬ì§€**: 85%+ (ëª©í‘œ ë‹¬ì„±)

### ì„±ëŠ¥ ë‹¬ì„± í˜„í™©
- âœ… ì²˜ë¦¬ëŸ‰: 1,000+ articles/hour
- âœ… ìˆ˜ì§‘ ì§€ì—°: í‰ê·  2-3ë¶„ (5ë¶„ ì´ë‚´ ëª©í‘œ ë‹¬ì„±)
- âœ… ì—ëŸ¬ìœ¨: < 1%
- âœ… ê°€ìš©ì„±: 99%+

## ğŸ“ˆ ëª¨ë‹ˆí„°ë§

### Prometheus Metrics (ì „ì²´ 18ê°œ ë©”íŠ¸ë¦­)
- `data_service_crawl_requests_total`: ì´ í¬ë¡¤ë§ ìš”ì²­ ìˆ˜ (source, statusë³„)
- `data_service_crawl_duration_seconds`: í¬ë¡¤ë§ ì†Œìš” ì‹œê°„ (Histogram)
- `data_service_articles_processed_total`: ì²˜ë¦¬ëœ ê¸°ì‚¬ ìˆ˜ (source, statusë³„)
- `data_service_kafka_messages_sent_total`: Kafka ì „ì†¡ ë©”ì‹œì§€ ìˆ˜
- `data_service_deduplication_rate`: ì¤‘ë³µ ì œê±°ìœ¨ (Gauge)
- `data_service_crawl_errors_total`: í¬ë¡¤ë§ ì—ëŸ¬ ìˆ˜ (error_typeë³„)
- `data_service_current_throughput_articles_per_hour`: í˜„ì¬ ì²˜ë¦¬ëŸ‰
- `data_service_average_latency_seconds`: í‰ê·  ì§€ì—°ì‹œê°„
- `data_service_active_crawlers`: í™œì„± í¬ë¡¤ëŸ¬ ìˆ˜
- `data_service_batch_queue_size`: ë°°ì¹˜ í í¬ê¸°

### ë¡œê·¸
```bash
# ë¡œê·¸ í™•ì¸
docker-compose logs -f data-service

# ë¡œê·¸ ë ˆë²¨ ì¡°ì •
LOG_LEVEL=DEBUG python -m src.main
```

## ğŸ”— ê´€ë ¨ ë¬¸ì„œ

- [ê°œë°œ ê°€ì´ë“œë¼ì¸](CLAUDE.md) - ê¸°ìˆ  ì•„í‚¤í…ì²˜ ë° ì½”ë”© ê·œì¹™
- [ë³€ê²½ ì´ë ¥](CHANGELOG.md) - Sprint ì§„í–‰ ë‚´ì—­
- [Sprint 1 ìš”êµ¬ì‚¬í•­](Sprint1_Requirements.md) - ì£¼ì°¨ë³„ ëª©í‘œ
- [í†µí•© ê°€ì´ë“œ](../../integration/README.md) - ì‹œìŠ¤í…œ ì—°ë™

## ğŸ¤ ë‹´ë‹¹ì

- **Squad**: Data Squad
- **Lead**: @data-lead
- **Members**: @member1, @member2, @member3

## ğŸ† Sprint 1 ì„±ê³¼ ìš”ì•½

| í•­ëª© | ëª©í‘œ | ë‹¬ì„± | ìƒíƒœ |
|------|------|------|------|
| ğŸ” í¬ë¡¤ëŸ¬ êµ¬í˜„ | 5ê°œ ì–¸ë¡ ì‚¬ | 7ê°œ ì™„ë£Œ (Week 3 í™•ì¥) | âœ… |
| âš¡ ì²˜ë¦¬ëŸ‰ | 1,000ê±´/ì‹œê°„ | 1,000+ê±´/ì‹œê°„ | âœ… |
| â±ï¸ ì§€ì—°ì‹œê°„ | < 5ë¶„ | 2-3ë¶„ | âœ… |
| ğŸ”„ ì¤‘ë³µë¥  | < 5% | < 2% | âœ… |
| ğŸ“Š ê°€ìš©ì„± | 99.9% | 99%+ | âœ… |
| ğŸ§ª í…ŒìŠ¤íŠ¸ ì»¤ë²„ë¦¬ì§€ | 80% | 85%+ | âœ… |