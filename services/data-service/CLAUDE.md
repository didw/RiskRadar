# Data Service Development Guidelines
# ë°ì´í„° ì„œë¹„ìŠ¤ ê°œë°œ ê°€ì´ë“œë¼ì¸

## ğŸ“‹ ì„œë¹„ìŠ¤ ê°œìš”

Data ServiceëŠ” RiskRadarì˜ ë°ì´í„° ìˆ˜ì§‘ ë° ì „ì²˜ë¦¬ë¥¼ ë‹´ë‹¹í•˜ëŠ” ë§ˆì´í¬ë¡œì„œë¹„ìŠ¤ì…ë‹ˆë‹¤. ë‰´ìŠ¤, ê³µì‹œ, SNS ë“± ë‹¤ì–‘í•œ ì†ŒìŠ¤ì—ì„œ ë°ì´í„°ë¥¼ ìˆ˜ì§‘í•˜ê³  Kafkaë¡œ ìŠ¤íŠ¸ë¦¬ë°í•©ë‹ˆë‹¤.

## ğŸ—ï¸ í”„ë¡œì íŠ¸ êµ¬ì¡°

```
data-service/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ crawlers/                # í¬ë¡¤ëŸ¬ êµ¬í˜„
â”‚   â”‚   â”œâ”€â”€ base_crawler.py      # ë² ì´ìŠ¤ í¬ë¡¤ëŸ¬ í´ë˜ìŠ¤
â”‚   â”‚   â”œâ”€â”€ news/                # ë‰´ìŠ¤ í¬ë¡¤ëŸ¬
â”‚   â”‚   â”‚   â”œâ”€â”€ chosun_crawler.py
â”‚   â”‚   â”‚   â”œâ”€â”€ hankyung_crawler.py
â”‚   â”‚   â”‚   â”œâ”€â”€ joongang_crawler.py
â”‚   â”‚   â”‚   â”œâ”€â”€ yonhap_crawler.py
â”‚   â”‚   â”‚   â””â”€â”€ mk_crawler.py
â”‚   â”‚   â””â”€â”€ disclosure/          # ê³µì‹œ í¬ë¡¤ëŸ¬ (ì˜ˆì •)
â”‚   â”œâ”€â”€ kafka/                   # Kafka ê´€ë ¨
â”‚   â”‚   â”œâ”€â”€ producer.py          # (ì˜ˆì •)
â”‚   â”‚   â””â”€â”€ schemas.py           # ë©”ì‹œì§€ ìŠ¤í‚¤ë§ˆ
â”‚   â”œâ”€â”€ processors/              # ë°ì´í„° ì „ì²˜ë¦¬ (ì˜ˆì •)
â”‚   â”‚   â”œâ”€â”€ cleaner.py
â”‚   â”‚   â”œâ”€â”€ validator.py
â”‚   â”‚   â””â”€â”€ enricher.py
â”‚   â”œâ”€â”€ api/                     # REST API (ì˜ˆì •)
â”‚   â”‚   â”œâ”€â”€ routes.py
â”‚   â”‚   â””â”€â”€ models.py
â”‚   â””â”€â”€ config.py                # ì„¤ì •
â”œâ”€â”€ tests/                       # í…ŒìŠ¤íŠ¸
â”‚   â”œâ”€â”€ unit/                    # ë‹¨ìœ„ í…ŒìŠ¤íŠ¸
â”‚   â””â”€â”€ integration/             # í†µí•© í…ŒìŠ¤íŠ¸
â”œâ”€â”€ scripts/                     # ìœ í‹¸ë¦¬í‹° ìŠ¤í¬ë¦½íŠ¸
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ requirements-dev.txt
â”œâ”€â”€ Dockerfile
â”œâ”€â”€ README.md
â”œâ”€â”€ CLAUDE.md                    # í˜„ì¬ íŒŒì¼
â””â”€â”€ CHANGELOG.md
```

## ğŸ’» ê°œë°œ í™˜ê²½ ì„¤ì •

### Prerequisites
```bash
Python 3.11+
Poetry ë˜ëŠ” pip
Docker
Redis (ë¡œì»¬ ê°œë°œìš©)
```

### ì„¤ì¹˜
```bash
# ê°€ìƒí™˜ê²½ ìƒì„±
python -m venv venv
source venv/bin/activate  # Windows: venv\Scripts\activate

# ì˜ì¡´ì„± ì„¤ì¹˜
pip install -r requirements.txt
pip install -r requirements-dev.txt

# í™˜ê²½ ë³€ìˆ˜ ì„¤ì •
cp .env.example .env
```

### ë¡œì»¬ ì‹¤í–‰
```bash
# Kafka ì‹œì‘ (docker-compose ì‚¬ìš©)
docker-compose up -d kafka zookeeper

# ì„œë¹„ìŠ¤ ì‹¤í–‰
python -m src.main

# ë˜ëŠ” ê°œë°œ ëª¨ë“œ
uvicorn src.main:app --reload --port 8001
```

## ğŸ”§ ì£¼ìš” ì»´í¬ë„ŒíŠ¸

### 1. Crawler Base Class
```python
from abc import ABC, abstractmethod
from typing import List, Dict, Any

class BaseCrawler(ABC):
    """ëª¨ë“  í¬ë¡¤ëŸ¬ì˜ ë² ì´ìŠ¤ í´ë˜ìŠ¤"""
    
    def __init__(self, source_id: str, base_url: str, 
                 rate_limit: float = 1.0, timeout: int = 30,
                 max_retries: int = 3):
        self.source_id = source_id
        self.base_url = base_url
        self.rate_limiter = RateLimiter(requests_per_second=rate_limit)
        self.max_retries = max_retries
        # ... ê¸°íƒ€ ì„¤ì •
    
    @abstractmethod
    async def fetch_article_list(self) -> List[str]:
        """ê¸°ì‚¬ URL ëª©ë¡ ìˆ˜ì§‘ - êµ¬í˜„ í•„ìš”"""
        pass
    
    @abstractmethod
    async def parse_article(self, url: str, html: str) -> Dict[str, Any]:
        """ê¸°ì‚¬ ë‚´ìš© íŒŒì‹± - êµ¬í˜„ í•„ìš”"""
        pass
    
    async def fetch_articles(self, max_articles: Optional[int] = None) -> List[Dict[str, Any]]:
        """ë©”ì¸ í¬ë¡¤ë§ ë©”ì„œë“œ - ê³µí†µ ë¡œì§ êµ¬í˜„"""
        # URL ìˆ˜ì§‘ â†’ ê²€ì¦ â†’ ê°œë³„ í¬ë¡¤ë§ â†’ ì •ê·œí™”
```

### 2. Kafka Producer
```python
class NewsProducer:
    """ë‰´ìŠ¤ ë°ì´í„°ë¥¼ Kafkaë¡œ ì „ì†¡"""
    
    def __init__(self):
        self.producer = KafkaProducer(
            bootstrap_servers=KAFKA_SERVERS,
            value_serializer=lambda v: json.dumps(v).encode('utf-8')
        )
    
    async def send_news(self, news_data: NewsModel):
        """ë‰´ìŠ¤ ë°ì´í„° ì „ì†¡"""
        try:
            future = self.producer.send('raw-news', news_data.dict())
            await future
        except Exception as e:
            logger.error(f"Failed to send news: {e}")
            raise
```

### 3. Data Models
```python
from pydantic import BaseModel, Field
from datetime import datetime

class NewsModel(BaseModel):
    """ë‰´ìŠ¤ ë°ì´í„° ëª¨ë¸"""
    id: str = Field(..., description="ê³ ìœ  ID")
    title: str = Field(..., description="ì œëª©")
    content: str = Field(..., description="ë³¸ë¬¸")
    source: str = Field(..., description="ì¶œì²˜")
    url: str = Field(..., description="ì›ë³¸ URL")
    published_at: datetime = Field(..., description="ë°œí–‰ì¼ì‹œ")
    crawled_at: datetime = Field(default_factory=datetime.now)
    metadata: Dict[str, Any] = Field(default_factory=dict)
```

## ğŸ“ ì½”ë”© ê·œì¹™

### 1. í¬ë¡¤ëŸ¬ êµ¬í˜„
- ëª¨ë“  í¬ë¡¤ëŸ¬ëŠ” `BaseCrawler`ë¥¼ ìƒì†
- ë¹„ë™ê¸° ì²˜ë¦¬ ì‚¬ìš© (`async/await`)
- Rate limiting ì¤€ìˆ˜
- User-Agent ì„¤ì • í•„ìˆ˜
- ì—ëŸ¬ ì²˜ë¦¬ ë° ì¬ì‹œë„ ë¡œì§ êµ¬í˜„

### 2. ë°ì´í„° ê²€ì¦
- Pydantic ëª¨ë¸ ì‚¬ìš©
- í•„ìˆ˜ í•„ë“œ ê²€ì¦
- URL ì •ê·œí™”
- ì¤‘ë³µ ì œê±° (URL ê¸°ë°˜)

### 3. ì—ëŸ¬ ì²˜ë¦¬
```python
try:
    articles = await crawler.fetch_articles()
except RateLimitError:
    await asyncio.sleep(60)
    # ì¬ì‹œë„
except NetworkError as e:
    logger.error(f"Network error: {e}")
    # ì•Œë¦¼ ì „ì†¡
except Exception as e:
    logger.exception("Unexpected error")
    # ëª¨ë‹ˆí„°ë§ ì‹œìŠ¤í…œì— ì•Œë¦¼
```

### 4. ë¡œê¹…
```python
import structlog

logger = structlog.get_logger()

# êµ¬ì¡°í™”ëœ ë¡œê¹… ì‚¬ìš©
logger.info("article_crawled", 
    source=source_id,
    url=article_url,
    title=article_title,
    timestamp=datetime.now()
)
```

## ğŸ§ª í…ŒìŠ¤íŠ¸

### ë‹¨ìœ„ í…ŒìŠ¤íŠ¸
```bash
pytest tests/unit/
```

### í†µí•© í…ŒìŠ¤íŠ¸
```bash
pytest tests/integration/
```

### í…ŒìŠ¤íŠ¸ ì‘ì„± ê·œì¹™
```python
class TestChosunCrawler:
    @pytest.fixture
    def crawler(self):
        return ChosunCrawler()
    
    async def test_fetch_articles(self, crawler, mock_response):
        # Given
        mock_response.return_value = sample_html
        
        # When
        articles = await crawler.fetch_articles()
        
        # Then
        assert len(articles) > 0
        assert all('title' in a for a in articles)
```

## ğŸš€ ë°°í¬

### Docker ë¹Œë“œ
```bash
docker build -t riskradar/data-service:latest .
```

### í™˜ê²½ ë³€ìˆ˜
```env
# Kafka
KAFKA_BOOTSTRAP_SERVERS=kafka:9092
KAFKA_TOPIC_RAW_NEWS=raw-news

# Redis
REDIS_URL=redis://redis:6379

# Crawler ì„¤ì •
CRAWLER_USER_AGENT="RiskRadar/1.0"
CRAWLER_TIMEOUT=30
CRAWLER_MAX_RETRIES=3

# API
API_PORT=8001
API_WORKERS=4
```

## ğŸ“Š ëª¨ë‹ˆí„°ë§

### Health Check
```python
@app.get("/health")
async def health_check():
    return {
        "status": "healthy",
        "timestamp": datetime.now(),
        "kafka": check_kafka_connection(),
        "redis": check_redis_connection()
    }
```

### Metrics
- í¬ë¡¤ë§ëœ ê¸°ì‚¬ ìˆ˜/ì‹œê°„
- í¬ë¡¤ë§ ì‹¤íŒ¨ìœ¨
- Kafka ì „ì†¡ ì§€ì—°ì‹œê°„
- API ì‘ë‹µ ì‹œê°„

## ğŸ”’ ë³´ì•ˆ

### í¬ë¡¤ë§ ìœ¤ë¦¬
- robots.txt ì¤€ìˆ˜
- ì ì ˆí•œ ë”œë ˆì´ ì„¤ì •
- ì„œë²„ ë¶€í•˜ ìµœì†Œí™”
- ì €ì‘ê¶Œ ê³ ë ¤

### API ë³´ì•ˆ
- Rate limiting
- API key ì¸ì¦
- Input validation
- SQL injection ë°©ì§€

## ğŸ¤ í˜‘ì—…

### ë‹¤ë¥¸ ì„œë¹„ìŠ¤ì™€ì˜ ì¸í„°í˜ì´ìŠ¤
- **Output**: Kafka topic `raw-news`
- **Schema**: [NewsModel](#3-data-models)
- **Format**: JSON
- **Encoding**: UTF-8

### ì˜ì¡´ì„±
- ML Serviceê°€ `raw-news` í† í”½ êµ¬ë…
- Graph Serviceê°€ ì²˜ë¦¬ ê²°ê³¼ ì €ì¥

## ğŸ› íŠ¸ëŸ¬ë¸”ìŠˆíŒ…

### ì¼ë°˜ì ì¸ ë¬¸ì œ

#### 1. Kafka ì—°ê²° ì‹¤íŒ¨
```bash
# Kafka ìƒíƒœ í™•ì¸
docker-compose ps kafka

# í† í”½ ëª©ë¡ í™•ì¸
kafka-topics --list --bootstrap-server localhost:9092
```

#### 2. í¬ë¡¤ë§ ì°¨ë‹¨
- User-Agent í™•ì¸
- IP ì°¨ë‹¨ ì—¬ë¶€ í™•ì¸
- Rate limit ì¡°ì •

#### 3. ë©”ëª¨ë¦¬ ëˆ„ìˆ˜
- í¬ë¡¤ëŸ¬ ê°ì²´ ì¬ì‚¬ìš©
- ì—°ê²° í’€ ê´€ë¦¬
- ê°€ë¹„ì§€ ì»¬ë ‰ì…˜ ëª¨ë‹ˆí„°ë§

## ğŸ“š ì°¸ê³  ìë£Œ

- [Apache Kafka Python Client](https://kafka-python.readthedocs.io/)
- [BeautifulSoup Documentation](https://www.crummy.com/software/BeautifulSoup/bs4/doc/)
- [FastAPI Documentation](https://fastapi.tiangolo.com/)
- [Pydantic Documentation](https://pydantic-docs.helpmanual.io/)

## ğŸ¯ Sprint ê°œë°œ ê°€ì´ë“œ

í˜„ì¬ Sprintì˜ ìƒì„¸ ìš”êµ¬ì‚¬í•­ì€ ë‹¤ìŒ ë¬¸ì„œë¥¼ ì°¸ê³ í•˜ì„¸ìš”:
- [Sprint 1 Requirements](./Sprint1_Requirements.md) - Weekë³„ êµ¬í˜„ ëª©í‘œ
- [Sprint Breakdown](../../docs/trd/phase1/Sprint_Breakdown.md) - ì „ì²´ Sprint ê³„íš

### ê°œë°œ ìš°ì„ ìˆœìœ„
1. ~~ë² ì´ìŠ¤ í¬ë¡¤ëŸ¬ í´ë˜ìŠ¤ êµ¬í˜„~~ âœ… ì™„ë£Œ (Week 1)
2. ~~5ê°œ ì£¼ìš” ì–¸ë¡ ì‚¬ í¬ë¡¤ëŸ¬ êµ¬í˜„~~ âœ… ì™„ë£Œ (Week 2)
   - ì¡°ì„ ì¼ë³´ (ChosunCrawler)
   - í•œêµ­ê²½ì œ (HankyungCrawler)
   - ì¤‘ì•™ì¼ë³´ (JoongangCrawler)
   - ì—°í•©ë‰´ìŠ¤ (YonhapCrawler)
   - ë§¤ì¼ê²½ì œ (MKCrawler)
3. ~~í†µí•© í…ŒìŠ¤íŠ¸ ì‘ì„±~~ âœ… ì™„ë£Œ (Week 2)
4. Kafka Producer êµ¬í˜„ (Week 3)
5. Bloom Filter ê¸°ë°˜ ì¤‘ë³µ ì œê±° (Week 3)
6. ë°°ì¹˜ ì²˜ë¦¬ ë° ìŠ¤ì¼€ì¤„ëŸ¬ êµ¬í˜„ (Week 3)

## ğŸ“ í”„ë¡œì íŠ¸ ë¬¸ì„œ

### í•µì‹¬ ë¬¸ì„œ
- [Data Squad TRD](../../docs/trd/phase1/TRD_Data_Squad_P1.md) - ê¸°ìˆ  ëª…ì„¸
- [API í‘œì¤€](../../docs/trd/common/API_Standards.md) - API ì„¤ê³„
- [ë°ì´í„° ëª¨ë¸](../../docs/trd/common/Data_Models.md) - ê³µí†µ êµ¬ì¡°

### ì—°ê´€ ì„œë¹„ìŠ¤
- [ML Service](../ml-service/CLAUDE.md) - Kafka ë©”ì‹œì§€ ìˆ˜ì‹ 
- [Graph Service](../graph-service/CLAUDE.md) - ì²˜ë¦¬ëœ ë°ì´í„° ì €ì¥
- [í†µí•© ê°€ì´ë“œ](../../integration/README.md) - ì‹œìŠ¤í…œ í†µí•©