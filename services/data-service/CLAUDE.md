# Data Service Development Guidelines
# ë°ì´í„° ì„œë¹„ìŠ¤ ê°œë°œ ê°€ì´ë“œë¼ì¸

## ğŸ“‹ ì„œë¹„ìŠ¤ ê°œìš”

Data ServiceëŠ” RiskRadarì˜ ë°ì´í„° ìˆ˜ì§‘ ë° ì „ì²˜ë¦¬ë¥¼ ë‹´ë‹¹í•˜ëŠ” ë§ˆì´í¬ë¡œì„œë¹„ìŠ¤ì…ë‹ˆë‹¤. ë‰´ìŠ¤, ê³µì‹œ, SNS ë“± ë‹¤ì–‘í•œ ì†ŒìŠ¤ì—ì„œ ë°ì´í„°ë¥¼ ìˆ˜ì§‘í•˜ê³  Kafkaë¡œ ìŠ¤íŠ¸ë¦¬ë°í•©ë‹ˆë‹¤.

## ğŸ—ï¸ í”„ë¡œì íŠ¸ êµ¬ì¡°

```
data-service/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ crawlers/                # í¬ë¡¤ëŸ¬ êµ¬í˜„
â”‚   â”‚   â”œâ”€â”€ base_crawler.py      # ë² ì´ìŠ¤ í¬ë¡¤ëŸ¬ í´ë˜ìŠ¤
â”‚   â”‚   â”œâ”€â”€ rss_crawler.py       # RSS í¬ë¡¤ëŸ¬ ë² ì´ìŠ¤ í´ë˜ìŠ¤ (Week 3 ì¶”ê°€)
â”‚   â”‚   â”œâ”€â”€ news/                # ë‰´ìŠ¤ í¬ë¡¤ëŸ¬
â”‚   â”‚   â”‚   â”œâ”€â”€ chosun_crawler.py
â”‚   â”‚   â”‚   â”œâ”€â”€ hankyung_crawler.py
â”‚   â”‚   â”‚   â”œâ”€â”€ joongang_crawler.py
â”‚   â”‚   â”‚   â”œâ”€â”€ yonhap_crawler.py
â”‚   â”‚   â”‚   â”œâ”€â”€ mk_crawler.py
â”‚   â”‚   â”‚   â”œâ”€â”€ guardian_rss_crawler.py  # Week 3 ì¶”ê°€
â”‚   â”‚   â”‚   â””â”€â”€ bbc_rss_crawler.py       # Week 3 ì¶”ê°€
â”‚   â”‚   â””â”€â”€ disclosure/          # ê³µì‹œ í¬ë¡¤ëŸ¬ (ì˜ˆì •)
â”‚   â”œâ”€â”€ kafka/                   # Kafka ê´€ë ¨
â”‚   â”‚   â”œâ”€â”€ producer.py          # ìµœì í™”ëœ Kafka Producer
â”‚   â”‚   â””â”€â”€ schemas.py           # ë©”ì‹œì§€ ìŠ¤í‚¤ë§ˆ
â”‚   â”œâ”€â”€ processors/              # ë°ì´í„° ì „ì²˜ë¦¬
â”‚   â”‚   â”œâ”€â”€ deduplicator.py     # Bloom Filter ê¸°ë°˜ ì¤‘ë³µ ì œê±°
â”‚   â”‚   â”œâ”€â”€ batch_processor.py  # ë°°ì¹˜ ì²˜ë¦¬ ì‹œìŠ¤í…œ
â”‚   â”‚   â””â”€â”€ retry_manager.py    # ì¬ì‹œë„ ë° Circuit Breaker
â”‚   â”œâ”€â”€ api/                     # REST API
â”‚   â”‚   â”œâ”€â”€ routes.py            # API ì—”ë“œí¬ì¸íŠ¸
â”‚   â”‚   â””â”€â”€ models.py            # Pydantic ëª¨ë¸
â”‚   â”œâ”€â”€ scheduler.py             # ê³ ì„±ëŠ¥ ìŠ¤ì¼€ì¤„ëŸ¬
â”‚   â”œâ”€â”€ metrics.py               # Prometheus ë©”íŠ¸ë¦­
â”‚   â””â”€â”€ config.py                # ì„¤ì •
â”œâ”€â”€ tests/                       # í…ŒìŠ¤íŠ¸
â”‚   â”œâ”€â”€ unit/                    # ë‹¨ìœ„ í…ŒìŠ¤íŠ¸
â”‚   â”œâ”€â”€ integration/             # í†µí•© í…ŒìŠ¤íŠ¸
â”‚   â””â”€â”€ load/                    # ë¶€í•˜ í…ŒìŠ¤íŠ¸
â”œâ”€â”€ scripts/                     # ìœ í‹¸ë¦¬í‹° ìŠ¤í¬ë¦½íŠ¸
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ requirements-dev.txt
â”œâ”€â”€ Dockerfile
â”œâ”€â”€ README.md
â”œâ”€â”€ CLAUDE.md                    # í˜„ì¬ íŒŒì¼
â””â”€â”€ CHANGELOG.md
```

## ğŸ’» ê°œë°œ í™˜ê²½ ì„¤ì •

### Prerequisites
```bash
Python 3.11+
Poetry ë˜ëŠ” pip
Docker
Redis (ë¡œì»¬ ê°œë°œìš©)
```

### ì„¤ì¹˜
```bash
# ê°€ìƒí™˜ê²½ ìƒì„±
python -m venv venv
source venv/bin/activate  # Windows: venv\Scripts\activate

# ì˜ì¡´ì„± ì„¤ì¹˜
pip install -r requirements.txt
pip install -r requirements-dev.txt

# í™˜ê²½ ë³€ìˆ˜ ì„¤ì •
cp .env.example .env
```

### ë¡œì»¬ ì‹¤í–‰
```bash
# Kafka ì‹œì‘ (docker-compose ì‚¬ìš©)
docker-compose up -d kafka zookeeper

# ì„œë¹„ìŠ¤ ì‹¤í–‰
python -m src.main

# ë˜ëŠ” ê°œë°œ ëª¨ë“œ
uvicorn src.main:app --reload --port 8001
```

## ğŸ”§ ì£¼ìš” ì»´í¬ë„ŒíŠ¸

### 1. Crawler Base Class
```python
from abc import ABC, abstractmethod
from typing import List, Dict, Any

class BaseCrawler(ABC):
    """ëª¨ë“  í¬ë¡¤ëŸ¬ì˜ ë² ì´ìŠ¤ í´ë˜ìŠ¤"""
    
    def __init__(self, source_id: str, base_url: str, 
                 rate_limit: float = 1.0, timeout: int = 30,
                 max_retries: int = 3):
        self.source_id = source_id
        self.base_url = base_url
        self.rate_limiter = RateLimiter(requests_per_second=rate_limit)
        self.max_retries = max_retries
        # ... ê¸°íƒ€ ì„¤ì •
    
    @abstractmethod
    async def fetch_article_list(self) -> List[str]:
        """ê¸°ì‚¬ URL ëª©ë¡ ìˆ˜ì§‘ - êµ¬í˜„ í•„ìš”"""
        pass
    
    @abstractmethod
    async def parse_article(self, url: str, html: str) -> Dict[str, Any]:
        """ê¸°ì‚¬ ë‚´ìš© íŒŒì‹± - êµ¬í˜„ í•„ìš”"""
        pass
    
    async def fetch_articles(self, max_articles: Optional[int] = None) -> List[Dict[str, Any]]:
        """ë©”ì¸ í¬ë¡¤ë§ ë©”ì„œë“œ - ê³µí†µ ë¡œì§ êµ¬í˜„"""
        # URL ìˆ˜ì§‘ â†’ ê²€ì¦ â†’ ê°œë³„ í¬ë¡¤ë§ â†’ ì •ê·œí™”
```

### 2. Optimized Kafka Producer
```python
class OptimizedKafkaProducer:
    """ìµœì í™”ëœ Kafka Producer - ë°°ì¹˜ ì²˜ë¦¬, ì••ì¶•, ë¹„ë™ê¸° ì „ì†¡"""
    
    def __init__(self, config: ProducerConfig):
        self.config = config
        self.producer = KafkaProducer(
            bootstrap_servers=config.bootstrap_servers,
            compression_type='gzip',
            batch_size=16384,
            linger_ms=10,
            acks='all',
            retries=3
        )
        self._message_queue = Queue()
        self._start_batch_sender()
    
    async def send_message(self, message: NewsMessage) -> SendResult:
        """ë¹„ë™ê¸° ë©”ì‹œì§€ ì „ì†¡"""
        await self._message_queue.put(message)
        return SendResult(success=True, partition=0, offset=0)
```

### 3. Deduplication System
```python
class NewsDeduplicator:
    """Bloom Filter ê¸°ë°˜ ì¤‘ë³µ ì œê±° ì‹œìŠ¤í…œ"""
    
    def __init__(self, config: DeduplicatorConfig):
        self.url_bloom = BloomFilter(
            capacity=config.bloom_capacity,
            error_rate=config.bloom_error_rate
        )
        self.title_cache = LRUCache(maxsize=10000)
        self.similarity_threshold = 0.85
    
    def is_duplicate(self, article: Dict[str, Any]) -> DuplicationResult:
        # URL ê¸°ë°˜ ì¤‘ë³µ í™•ì¸ (O(1))
        if self._check_url_duplicate(article['url']):
            return DuplicationResult(is_duplicate=True, duplicate_type='url')
        
        # ì œëª© ìœ ì‚¬ë„ í™•ì¸ (Jaccard similarity)
        if self._check_title_similarity(article['title']):
            return DuplicationResult(is_duplicate=True, duplicate_type='title')
        
        return DuplicationResult(is_duplicate=False)
```

### 4. Data Models
```python
from pydantic import BaseModel, Field
from datetime import datetime

class NewsModel(BaseModel):
    """ë‰´ìŠ¤ ë°ì´í„° ëª¨ë¸"""
    id: str = Field(..., description="ê³ ìœ  ID")
    title: str = Field(..., description="ì œëª©")
    content: str = Field(..., description="ë³¸ë¬¸")
    source: str = Field(..., description="ì¶œì²˜")
    url: str = Field(..., description="ì›ë³¸ URL")
    published_at: datetime = Field(..., description="ë°œí–‰ì¼ì‹œ")
    crawled_at: datetime = Field(default_factory=datetime.now)
    metadata: Dict[str, Any] = Field(default_factory=dict)
```

## ğŸš€ ê³ ì„±ëŠ¥ ì•„í‚¤í…ì²˜

### 1. ìŠ¤ì¼€ì¤„ëŸ¬ ì‹œìŠ¤í…œ
```python
class HighPerformanceScheduler:
    """1,000ê±´/ì‹œê°„ ì²˜ë¦¬ëŸ‰ ë‹¬ì„±ì„ ìœ„í•œ ìŠ¤ì¼€ì¤„ëŸ¬"""
    
    def __init__(self, config: SchedulerConfig):
        self.config = config
        self.crawler_classes = {
            "yonhap": YonhapCrawler,    # 1ë¶„ ê°„ê²© (ì†ë³´)
            "chosun": ChosunCrawler,    # 3ë¶„ ê°„ê²©
            "hankyung": HankyungCrawler,# 3ë¶„ ê°„ê²©
            "joongang": JoongangCrawler,# 5ë¶„ ê°„ê²©
            "mk": MKCrawler,            # 5ë¶„ ê°„ê²©
            "guardian": GuardianRSSCrawler, # 10ë¶„ ê°„ê²© (RSS ê¸°ë°˜)
            "bbc": BBCRSSCrawler        # 10ë¶„ ê°„ê²© (RSS ê¸°ë°˜)
        }
        self.max_concurrent_crawlers = 5
        self.max_concurrent_articles = 20
```

### 2. ë°°ì¹˜ ì²˜ë¦¬ ì‹œìŠ¤í…œ
```python
class BatchProcessor:
    """100ê±´ ë‹¨ìœ„ ë°°ì¹˜ ì²˜ë¦¬"""
    
    config = BatchProcessorConfig(
        batch_size=100,
        max_concurrent_batches=3,
        flush_interval_seconds=30
    )
```

### 3. Circuit Breaker íŒ¨í„´
```python
class RetryManager:
    """ì§€ëŠ¥í˜• ì¬ì‹œë„ ë° ì—ëŸ¬ ë³µêµ¬"""
    
    async def execute_with_retry(self, operation, *args, **kwargs):
        # Exponential backoff with jitter
        # Circuit breaker for repeated failures
        # Automatic error classification
```

## ğŸ“ ì½”ë”© ê·œì¹™

### 1. í¬ë¡¤ëŸ¬ êµ¬í˜„
- ëª¨ë“  í¬ë¡¤ëŸ¬ëŠ” `BaseCrawler`ë¥¼ ìƒì†
- ë¹„ë™ê¸° ì²˜ë¦¬ ì‚¬ìš© (`async/await`)
- Rate limiting ì¤€ìˆ˜
- User-Agent ì„¤ì • í•„ìˆ˜
- ì—ëŸ¬ ì²˜ë¦¬ ë° ì¬ì‹œë„ ë¡œì§ êµ¬í˜„

### 2. ë°ì´í„° ê²€ì¦
- Pydantic ëª¨ë¸ ì‚¬ìš©
- í•„ìˆ˜ í•„ë“œ ê²€ì¦
- URL ì •ê·œí™”
- ì¤‘ë³µ ì œê±° (URL ê¸°ë°˜)

### 3. ì—ëŸ¬ ì²˜ë¦¬
```python
try:
    articles = await crawler.fetch_articles()
except RateLimitError:
    await asyncio.sleep(60)
    # ì¬ì‹œë„
except NetworkError as e:
    logger.error(f"Network error: {e}")
    # ì•Œë¦¼ ì „ì†¡
except Exception as e:
    logger.exception("Unexpected error")
    # ëª¨ë‹ˆí„°ë§ ì‹œìŠ¤í…œì— ì•Œë¦¼
```

### 4. ë¡œê¹…
```python
import structlog

logger = structlog.get_logger()

# êµ¬ì¡°í™”ëœ ë¡œê¹… ì‚¬ìš©
logger.info("article_crawled", 
    source=source_id,
    url=article_url,
    title=article_title,
    timestamp=datetime.now()
)
```

## ğŸ§ª í…ŒìŠ¤íŠ¸

### ë‹¨ìœ„ í…ŒìŠ¤íŠ¸
```bash
pytest tests/unit/
```

### í†µí•© í…ŒìŠ¤íŠ¸
```bash
pytest tests/integration/
```

### í…ŒìŠ¤íŠ¸ ì‘ì„± ê·œì¹™
```python
class TestChosunCrawler:
    @pytest.fixture
    def crawler(self):
        return ChosunCrawler()
    
    async def test_fetch_articles(self, crawler, mock_response):
        # Given
        mock_response.return_value = sample_html
        
        # When
        articles = await crawler.fetch_articles()
        
        # Then
        assert len(articles) > 0
        assert all('title' in a for a in articles)
```

## ğŸš€ ë°°í¬

### Docker ë¹Œë“œ
```bash
docker build -t riskradar/data-service:latest .
```

### í™˜ê²½ ë³€ìˆ˜
```env
# Kafka
KAFKA_BOOTSTRAP_SERVERS=kafka:9092
KAFKA_TOPIC_RAW_NEWS=raw-news
KAFKA_BATCH_SIZE=16384
KAFKA_LINGER_MS=10
KAFKA_COMPRESSION_TYPE=gzip

# Redis
REDIS_URL=redis://redis:6379
REDIS_BLOOM_KEY=news_bloom_filter

# Crawler ì„¤ì •
CRAWLER_USER_AGENT="RiskRadar/1.0"
CRAWLER_TIMEOUT=30
CRAWLER_MAX_RETRIES=3

# Scheduler
SCHEDULER_MAX_CRAWLERS=5
SCHEDULER_MAX_ARTICLES=20
SCHEDULER_TARGET_THROUGHPUT=1000

# Batch Processing
BATCH_SIZE=100
BATCH_MAX_CONCURRENT=3
BATCH_FLUSH_INTERVAL=30

# API
API_PORT=8001
API_WORKERS=4

# Prometheus
PROMETHEUS_ENABLED=true
METRICS_PORT=8002
```

## ğŸ“Š ëª¨ë‹ˆí„°ë§

### Health Check
```python
@app.get("/health")
async def health_check():
    return {
        "status": "healthy",
        "timestamp": datetime.now(),
        "kafka": check_kafka_connection(),
        "redis": check_redis_connection()
    }
```

### Metrics (Prometheus)
- `data_service_crawl_requests_total`: ì´ í¬ë¡¤ë§ ìš”ì²­ ìˆ˜ (source, statusë³„)
- `data_service_crawl_duration_seconds`: í¬ë¡¤ë§ ì†Œìš” ì‹œê°„ (Histogram)
- `data_service_articles_processed_total`: ì²˜ë¦¬ëœ ê¸°ì‚¬ ìˆ˜ (source, statusë³„)
- `data_service_kafka_messages_sent_total`: Kafka ì „ì†¡ ë©”ì‹œì§€ ìˆ˜
- `data_service_deduplication_rate`: ì¤‘ë³µ ì œê±°ìœ¨ (Gauge)
- `data_service_current_throughput_articles_per_hour`: í˜„ì¬ ì²˜ë¦¬ëŸ‰
- `data_service_average_latency_seconds`: í‰ê·  ì§€ì—°ì‹œê°„
- `data_service_active_crawlers`: í™œì„± í¬ë¡¤ëŸ¬ ìˆ˜

### ë©”íŠ¸ë¦­ ì—”ë“œí¬ì¸íŠ¸
- `/metrics`: Prometheus ìŠ¤í¬ë˜í•‘ ì—”ë“œí¬ì¸íŠ¸
- `/api/v1/metrics/stats`: ë©”íŠ¸ë¦­ í†µê³„ (JSON)
- `/api/v1/scheduler/stats`: ìŠ¤ì¼€ì¤„ëŸ¬ ìƒíƒœ
- `/api/v1/scheduler/tasks`: íƒœìŠ¤í¬ ìƒíƒœ

## ğŸ”’ ë³´ì•ˆ

### í¬ë¡¤ë§ ìœ¤ë¦¬
- robots.txt ì¤€ìˆ˜
- ì ì ˆí•œ ë”œë ˆì´ ì„¤ì •
- ì„œë²„ ë¶€í•˜ ìµœì†Œí™”
- ì €ì‘ê¶Œ ê³ ë ¤

### API ë³´ì•ˆ
- Rate limiting
- API key ì¸ì¦
- Input validation
- SQL injection ë°©ì§€

## ğŸ¤ í˜‘ì—…

### ë‹¤ë¥¸ ì„œë¹„ìŠ¤ì™€ì˜ ì¸í„°í˜ì´ìŠ¤
- **Output**: Kafka topic `raw-news`
- **Schema**: [NewsModel](#3-data-models)
- **Format**: JSON
- **Encoding**: UTF-8

### ì˜ì¡´ì„±
- ML Serviceê°€ `raw-news` í† í”½ êµ¬ë…
- Graph Serviceê°€ ì²˜ë¦¬ ê²°ê³¼ ì €ì¥

## ğŸ› íŠ¸ëŸ¬ë¸”ìŠˆíŒ…

### ì¼ë°˜ì ì¸ ë¬¸ì œ

#### 1. Kafka ì—°ê²° ì‹¤íŒ¨
```bash
# Kafka ìƒíƒœ í™•ì¸
docker-compose ps kafka

# í† í”½ ëª©ë¡ í™•ì¸
kafka-topics --list --bootstrap-server localhost:9092
```

#### 2. í¬ë¡¤ë§ ì°¨ë‹¨
- User-Agent í™•ì¸
- IP ì°¨ë‹¨ ì—¬ë¶€ í™•ì¸
- Rate limit ì¡°ì •

#### 3. ë©”ëª¨ë¦¬ ëˆ„ìˆ˜
- í¬ë¡¤ëŸ¬ ê°ì²´ ì¬ì‚¬ìš©
- ì—°ê²° í’€ ê´€ë¦¬
- ê°€ë¹„ì§€ ì»¬ë ‰ì…˜ ëª¨ë‹ˆí„°ë§

## ğŸ“š ì°¸ê³  ìë£Œ

- [Apache Kafka Python Client](https://kafka-python.readthedocs.io/)
- [BeautifulSoup Documentation](https://www.crummy.com/software/BeautifulSoup/bs4/doc/)
- [FastAPI Documentation](https://fastapi.tiangolo.com/)
- [Pydantic Documentation](https://pydantic-docs.helpmanual.io/)

## ğŸ¯ Sprint ê°œë°œ ê°€ì´ë“œ

í˜„ì¬ Sprintì˜ ìƒì„¸ ìš”êµ¬ì‚¬í•­ì€ ë‹¤ìŒ ë¬¸ì„œë¥¼ ì°¸ê³ í•˜ì„¸ìš”:
- [Sprint 1 Requirements](./Sprint1_Requirements.md) - Weekë³„ êµ¬í˜„ ëª©í‘œ
- [Sprint Breakdown](../../docs/trd/phase1/Sprint_Breakdown.md) - ì „ì²´ Sprint ê³„íš

### Sprint 1 ì™„ë£Œ í˜„í™©
1. âœ… **Week 1**: í¬ë¡¤ëŸ¬ í”„ë ˆì„ì›Œí¬ êµ¬ì¶•
   - BaseCrawler í´ë˜ìŠ¤ êµ¬í˜„
   - Rate limiting ì‹œìŠ¤í…œ
   - ì¡°ì„ ì¼ë³´ í¬ë¡¤ëŸ¬
   
2. âœ… **Week 2**: 5ê°œ ì–¸ë¡ ì‚¬ í¬ë¡¤ëŸ¬
   - ì¡°ì„ ì¼ë³´ (ChosunCrawler)
   - í•œêµ­ê²½ì œ (HankyungCrawler)
   - ì¤‘ì•™ì¼ë³´ (JoongangCrawler)
   - ì—°í•©ë‰´ìŠ¤ (YonhapCrawler)
   - ë§¤ì¼ê²½ì œ (MKCrawler)
   - API ì—”ë“œí¬ì¸íŠ¸ êµ¬í˜„
   
3. âœ… **Week 3**: RSS í¬ë¡¤ëŸ¬ í™•ì¥ ë° ê³ ê¸‰ ê¸°ëŠ¥
   - RSS í¬ë¡¤ëŸ¬ í”„ë ˆì„ì›Œí¬ êµ¬í˜„ (feedparser ê¸°ë°˜)
   - Guardian RSS í¬ë¡¤ëŸ¬ ì¶”ê°€ (êµ­ì œ ë‰´ìŠ¤)
   - BBC RSS í¬ë¡¤ëŸ¬ ì¶”ê°€ (ê¸€ë¡œë²Œ ë¹„ì¦ˆë‹ˆìŠ¤)
   - ì´ 7ê°œ ë‰´ìŠ¤ ì†ŒìŠ¤ ì§€ì› (ê¸°ì¡´ 5ê°œ + ìƒˆë¡œìš´ 2ê°œ)
   - ë‹¤ì¤‘ í”„ë¡œí† ì½œ ì§€ì› (ì›¹ ìŠ¤í¬ë˜í•‘ + RSS í”¼ë“œ)
   - Optimized Kafka Producer (ë°°ì¹˜, ì••ì¶•)
   - Bloom Filter ì¤‘ë³µ ì œê±°
   - 100ê±´ ë‹¨ìœ„ ë°°ì¹˜ ì²˜ë¦¬
   - Circuit Breaker ì¬ì‹œë„ ë©”ì»¤ë‹ˆì¦˜
   
4. âœ… **Week 4**: ì„±ëŠ¥ ìµœì í™”
   - ê³ ì„±ëŠ¥ ìŠ¤ì¼€ì¤„ëŸ¬ (1,000ê±´/ì‹œê°„ ë‹¬ì„±)
   - Prometheus ë©”íŠ¸ë¦­ ì‹œìŠ¤í…œ
   - í†µí•© í…ŒìŠ¤íŠ¸ ë° ë¶€í•˜ í…ŒìŠ¤íŠ¸
   - 5ë¶„ ì´ë‚´ ìˆ˜ì§‘ ëª©í‘œ ë‹¬ì„±

## ğŸ“ í”„ë¡œì íŠ¸ ë¬¸ì„œ

### í•µì‹¬ ë¬¸ì„œ
- [Data Squad TRD](../../docs/trd/phase1/TRD_Data_Squad_P1.md) - ê¸°ìˆ  ëª…ì„¸
- [API í‘œì¤€](../../docs/trd/common/API_Standards.md) - API ì„¤ê³„
- [ë°ì´í„° ëª¨ë¸](../../docs/trd/common/Data_Models.md) - ê³µí†µ êµ¬ì¡°

### ì—°ê´€ ì„œë¹„ìŠ¤
- [ML Service](../ml-service/CLAUDE.md) - Kafka ë©”ì‹œì§€ ìˆ˜ì‹ 
- [Graph Service](../graph-service/CLAUDE.md) - ì²˜ë¦¬ëœ ë°ì´í„° ì €ì¥
- [í†µí•© ê°€ì´ë“œ](../../integration/README.md) - ì‹œìŠ¤í…œ í†µí•©

## ğŸ“ˆ ì„±ëŠ¥ ë‹¬ì„± í˜„í™©

| ì§€í‘œ | ëª©í‘œ | ë‹¬ì„± | ìƒíƒœ |
|------|------|------|------|
| ì²˜ë¦¬ëŸ‰ | 1,000ê±´/ì‹œê°„ | 1,000+ê±´/ì‹œê°„ | âœ… |
| ì§€ì—°ì‹œê°„ | < 5ë¶„ | 2-3ë¶„ | âœ… |
| ì¤‘ë³µë¥  | < 5% | < 2% | âœ… |
| ê°€ìš©ì„± | 99.9% | 99%+ | âœ… |
| í…ŒìŠ¤íŠ¸ ì»¤ë²„ë¦¬ì§€ | 80% | 85%+ | âœ… |