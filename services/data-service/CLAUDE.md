# Data Service Development Guidelines
# Îç∞Ïù¥ÌÑ∞ ÏÑúÎπÑÏä§ Í∞úÎ∞ú Í∞ÄÏù¥ÎìúÎùºÏù∏

## üìã ÏÑúÎπÑÏä§ Í∞úÏöî

Data ServiceÎäî RiskRadarÏùò Îç∞Ïù¥ÌÑ∞ ÏàòÏßë Î∞è Ï†ÑÏ≤òÎ¶¨Î•º Îã¥ÎãπÌïòÎäî ÎßàÏù¥ÌÅ¨Î°úÏÑúÎπÑÏä§ÏûÖÎãàÎã§. Îâ¥Ïä§, Í≥µÏãú, SNS Îì± Îã§ÏñëÌïú ÏÜåÏä§ÏóêÏÑú Îç∞Ïù¥ÌÑ∞Î•º ÏàòÏßëÌïòÍ≥† KafkaÎ°ú Ïä§Ìä∏Î¶¨Î∞çÌï©ÎãàÎã§.

## üèóÔ∏è ÌîÑÎ°úÏ†ùÌä∏ Íµ¨Ï°∞

```
data-service/
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îú‚îÄ‚îÄ crawlers/                # ÌÅ¨Î°§Îü¨ Íµ¨ÌòÑ
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ base_crawler.py      # Î≤†Ïù¥Ïä§ ÌÅ¨Î°§Îü¨ ÌÅ¥ÎûòÏä§
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ rss_crawler.py       # RSS ÌÅ¨Î°§Îü¨ Î≤†Ïù¥Ïä§ ÌÅ¥ÎûòÏä§ (Week 3 Ï∂îÍ∞Ä)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ news/                # Îâ¥Ïä§ ÌÅ¨Î°§Îü¨
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ chosun_crawler.py
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ hankyung_crawler.py
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ joongang_crawler.py
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ yonhap_crawler.py
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ mk_crawler.py
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ guardian_rss_crawler.py  # Week 3 Ï∂îÍ∞Ä
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ bbc_rss_crawler.py       # Week 3 Ï∂îÍ∞Ä
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ disclosure/          # Í≥µÏãú ÌÅ¨Î°§Îü¨ (ÏòàÏ†ï)
‚îÇ   ‚îú‚îÄ‚îÄ kafka/                   # Kafka Í¥ÄÎ†®
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ producer.py          # ÏµúÏ†ÅÌôîÎêú Kafka Producer
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ schemas.py           # Î©îÏãúÏßÄ Ïä§ÌÇ§Îßà
‚îÇ   ‚îú‚îÄ‚îÄ processors/              # Îç∞Ïù¥ÌÑ∞ Ï†ÑÏ≤òÎ¶¨
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ deduplicator.py     # Bloom Filter Í∏∞Î∞ò Ï§ëÎ≥µ Ï†úÍ±∞
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ batch_processor.py  # Î∞∞Ïπò Ï≤òÎ¶¨ ÏãúÏä§ÌÖú
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ retry_manager.py    # Ïû¨ÏãúÎèÑ Î∞è Circuit Breaker
‚îÇ   ‚îú‚îÄ‚îÄ api/                     # REST API
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ routes.py            # API ÏóîÎìúÌè¨Ïù∏Ìä∏
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ models.py            # Pydantic Î™®Îç∏
‚îÇ   ‚îú‚îÄ‚îÄ scheduler.py             # Í≥†ÏÑ±Îä• Ïä§ÏºÄÏ§ÑÎü¨
‚îÇ   ‚îú‚îÄ‚îÄ metrics.py               # Prometheus Î©îÌä∏Î¶≠
‚îÇ   ‚îî‚îÄ‚îÄ config.py                # ÏÑ§Ï†ï
‚îú‚îÄ‚îÄ tests/                       # ÌÖåÏä§Ìä∏
‚îÇ   ‚îú‚îÄ‚îÄ unit/                    # Îã®ÏúÑ ÌÖåÏä§Ìä∏
‚îÇ   ‚îú‚îÄ‚îÄ integration/             # ÌÜµÌï© ÌÖåÏä§Ìä∏
‚îÇ   ‚îî‚îÄ‚îÄ load/                    # Î∂ÄÌïò ÌÖåÏä§Ìä∏
‚îú‚îÄ‚îÄ scripts/                     # Ïú†Ìã∏Î¶¨Ìã∞ Ïä§ÌÅ¨Î¶ΩÌä∏
‚îú‚îÄ‚îÄ requirements.txt
‚îú‚îÄ‚îÄ requirements-dev.txt
‚îú‚îÄ‚îÄ Dockerfile
‚îú‚îÄ‚îÄ README.md
‚îú‚îÄ‚îÄ CLAUDE.md                    # ÌòÑÏû¨ ÌååÏùº
‚îî‚îÄ‚îÄ CHANGELOG.md
```

## üíª Í∞úÎ∞ú ÌôòÍ≤Ω ÏÑ§Ï†ï

### Prerequisites
```bash
Python 3.11+
Poetry ÎòêÎäî pip
Docker
Redis (Î°úÏª¨ Í∞úÎ∞úÏö©)
```

### ÏÑ§Ïπò
```bash
# Í∞ÄÏÉÅÌôòÍ≤Ω ÏÉùÏÑ±
python -m venv venv
source venv/bin/activate  # Windows: venv\Scripts\activate

# ÏùòÏ°¥ÏÑ± ÏÑ§Ïπò
pip install -r requirements.txt
pip install -r requirements-dev.txt

# ÌôòÍ≤Ω Î≥ÄÏàò ÏÑ§Ï†ï
cp .env.example .env
```

### Î°úÏª¨ Ïã§Ìñâ
```bash
# Kafka ÏãúÏûë (docker-compose ÏÇ¨Ïö©)
docker-compose up -d kafka zookeeper

# ÏÑúÎπÑÏä§ Ïã§Ìñâ
python -m src.main

# ÎòêÎäî Í∞úÎ∞ú Î™®Îìú
uvicorn src.main:app --reload --port 8001
```

## üîß Ï£ºÏöî Ïª¥Ìè¨ÎÑåÌä∏

### 1. Crawler Base Class
```python
from abc import ABC, abstractmethod
from typing import List, Dict, Any

class BaseCrawler(ABC):
    """Î™®Îì† ÌÅ¨Î°§Îü¨Ïùò Î≤†Ïù¥Ïä§ ÌÅ¥ÎûòÏä§"""
    
    def __init__(self, source_id: str, base_url: str, 
                 rate_limit: float = 1.0, timeout: int = 30,
                 max_retries: int = 3):
        self.source_id = source_id
        self.base_url = base_url
        self.rate_limiter = RateLimiter(requests_per_second=rate_limit)
        self.max_retries = max_retries
        # ... Í∏∞ÌÉÄ ÏÑ§Ï†ï
    
    @abstractmethod
    async def fetch_article_list(self) -> List[str]:
        """Í∏∞ÏÇ¨ URL Î™©Î°ù ÏàòÏßë - Íµ¨ÌòÑ ÌïÑÏöî"""
        pass
    
    @abstractmethod
    async def parse_article(self, url: str, html: str) -> Dict[str, Any]:
        """Í∏∞ÏÇ¨ ÎÇ¥Ïö© ÌååÏã± - Íµ¨ÌòÑ ÌïÑÏöî"""
        pass
    
    async def fetch_articles(self, max_articles: Optional[int] = None) -> List[Dict[str, Any]]:
        """Î©îÏù∏ ÌÅ¨Î°§ÎßÅ Î©îÏÑúÎìú - Í≥µÌÜµ Î°úÏßÅ Íµ¨ÌòÑ"""
        # URL ÏàòÏßë ‚Üí Í≤ÄÏ¶ù ‚Üí Í∞úÎ≥Ñ ÌÅ¨Î°§ÎßÅ ‚Üí Ï†ïÍ∑úÌôî
```

### 2. Optimized Kafka Producer
```python
class OptimizedKafkaProducer:
    """ÏµúÏ†ÅÌôîÎêú Kafka Producer - Î∞∞Ïπò Ï≤òÎ¶¨, ÏïïÏ∂ï, ÎπÑÎèôÍ∏∞ Ï†ÑÏÜ°"""
    
    def __init__(self, config: ProducerConfig):
        self.config = config
        self.producer = KafkaProducer(
            bootstrap_servers=config.bootstrap_servers,
            compression_type='gzip',
            batch_size=16384,
            linger_ms=10,
            acks='all',
            retries=3
        )
        self._message_queue = Queue()
        self._start_batch_sender()
    
    async def send_message(self, message: NewsMessage) -> SendResult:
        """ÎπÑÎèôÍ∏∞ Î©îÏãúÏßÄ Ï†ÑÏÜ°"""
        await self._message_queue.put(message)
        return SendResult(success=True, partition=0, offset=0)
```

### 3. Deduplication System
```python
class NewsDeduplicator:
    """Bloom Filter Í∏∞Î∞ò Ï§ëÎ≥µ Ï†úÍ±∞ ÏãúÏä§ÌÖú"""
    
    def __init__(self, config: DeduplicatorConfig):
        self.url_bloom = BloomFilter(
            capacity=config.bloom_capacity,
            error_rate=config.bloom_error_rate
        )
        self.title_cache = LRUCache(maxsize=10000)
        self.similarity_threshold = 0.85
    
    def is_duplicate(self, article: Dict[str, Any]) -> DuplicationResult:
        # URL Í∏∞Î∞ò Ï§ëÎ≥µ ÌôïÏù∏ (O(1))
        if self._check_url_duplicate(article['url']):
            return DuplicationResult(is_duplicate=True, duplicate_type='url')
        
        # Ï†úÎ™© Ïú†ÏÇ¨ÎèÑ ÌôïÏù∏ (Jaccard similarity)
        if self._check_title_similarity(article['title']):
            return DuplicationResult(is_duplicate=True, duplicate_type='title')
        
        return DuplicationResult(is_duplicate=False)
```

### 4. Data Models
```python
from pydantic import BaseModel, Field
from datetime import datetime

class NewsModel(BaseModel):
    """Îâ¥Ïä§ Îç∞Ïù¥ÌÑ∞ Î™®Îç∏"""
    id: str = Field(..., description="Í≥†Ïú† ID")
    title: str = Field(..., description="Ï†úÎ™©")
    content: str = Field(..., description="Î≥∏Î¨∏")
    source: str = Field(..., description="Ï∂úÏ≤ò")
    url: str = Field(..., description="ÏõêÎ≥∏ URL")
    published_at: datetime = Field(..., description="Î∞úÌñâÏùºÏãú")
    crawled_at: datetime = Field(default_factory=datetime.now)
    metadata: Dict[str, Any] = Field(default_factory=dict)
```

## üöÄ Í≥†ÏÑ±Îä• ÏïÑÌÇ§ÌÖçÏ≤ò

### 1. Ïä§ÏºÄÏ§ÑÎü¨ ÏãúÏä§ÌÖú
```python
class HighPerformanceScheduler:
    """1,000Í±¥/ÏãúÍ∞Ñ Ï≤òÎ¶¨Îüâ Îã¨ÏÑ±ÏùÑ ÏúÑÌïú Ïä§ÏºÄÏ§ÑÎü¨"""
    
    def __init__(self, config: SchedulerConfig):
        self.config = config
        self.crawler_classes = {
            "yonhap": YonhapCrawler,    # 1Î∂Ñ Í∞ÑÍ≤© (ÏÜçÎ≥¥)
            "chosun": ChosunCrawler,    # 3Î∂Ñ Í∞ÑÍ≤©
            "hankyung": HankyungCrawler,# 3Î∂Ñ Í∞ÑÍ≤©
            "joongang": JoongangCrawler,# 5Î∂Ñ Í∞ÑÍ≤©
            "mk": MKCrawler,            # 5Î∂Ñ Í∞ÑÍ≤©
            "guardian": GuardianRSSCrawler, # 10Î∂Ñ Í∞ÑÍ≤© (RSS Í∏∞Î∞ò)
            "bbc": BBCRSSCrawler        # 10Î∂Ñ Í∞ÑÍ≤© (RSS Í∏∞Î∞ò)
        }
        self.max_concurrent_crawlers = 5
        self.max_concurrent_articles = 20
```

### 2. Î∞∞Ïπò Ï≤òÎ¶¨ ÏãúÏä§ÌÖú
```python
class BatchProcessor:
    """100Í±¥ Îã®ÏúÑ Î∞∞Ïπò Ï≤òÎ¶¨"""
    
    config = BatchProcessorConfig(
        batch_size=100,
        max_concurrent_batches=3,
        flush_interval_seconds=30
    )
```

### 3. Circuit Breaker Ìå®ÌÑ¥
```python
class RetryManager:
    """ÏßÄÎä•Ìòï Ïû¨ÏãúÎèÑ Î∞è ÏóêÎü¨ Î≥µÍµ¨"""
    
    async def execute_with_retry(self, operation, *args, **kwargs):
        # Exponential backoff with jitter
        # Circuit breaker for repeated failures
        # Automatic error classification
```

## üìù ÏΩîÎî© Í∑úÏπô

### 1. ÌÅ¨Î°§Îü¨ Íµ¨ÌòÑ
- Î™®Îì† ÌÅ¨Î°§Îü¨Îäî `BaseCrawler`Î•º ÏÉÅÏÜç
- ÎπÑÎèôÍ∏∞ Ï≤òÎ¶¨ ÏÇ¨Ïö© (`async/await`)
- Rate limiting Ï§ÄÏàò
- User-Agent ÏÑ§Ï†ï ÌïÑÏàò
- ÏóêÎü¨ Ï≤òÎ¶¨ Î∞è Ïû¨ÏãúÎèÑ Î°úÏßÅ Íµ¨ÌòÑ

### 2. Îç∞Ïù¥ÌÑ∞ Í≤ÄÏ¶ù
- Pydantic Î™®Îç∏ ÏÇ¨Ïö©
- ÌïÑÏàò ÌïÑÎìú Í≤ÄÏ¶ù
- URL Ï†ïÍ∑úÌôî
- Ï§ëÎ≥µ Ï†úÍ±∞ (URL Í∏∞Î∞ò)

### 3. ÏóêÎü¨ Ï≤òÎ¶¨
```python
try:
    articles = await crawler.fetch_articles()
except RateLimitError:
    await asyncio.sleep(60)
    # Ïû¨ÏãúÎèÑ
except NetworkError as e:
    logger.error(f"Network error: {e}")
    # ÏïåÎ¶º Ï†ÑÏÜ°
except Exception as e:
    logger.exception("Unexpected error")
    # Î™®ÎãàÌÑ∞ÎßÅ ÏãúÏä§ÌÖúÏóê ÏïåÎ¶º
```

### 4. Î°úÍπÖ
```python
import structlog

logger = structlog.get_logger()

# Íµ¨Ï°∞ÌôîÎêú Î°úÍπÖ ÏÇ¨Ïö©
logger.info("article_crawled", 
    source=source_id,
    url=article_url,
    title=article_title,
    timestamp=datetime.now()
)
```

## üß™ ÌÖåÏä§Ìä∏

### Îã®ÏúÑ ÌÖåÏä§Ìä∏
```bash
pytest tests/unit/
```

### ÌÜµÌï© ÌÖåÏä§Ìä∏
```bash
pytest tests/integration/
```

### ÌÖåÏä§Ìä∏ ÏûëÏÑ± Í∑úÏπô
```python
class TestChosunCrawler:
    @pytest.fixture
    def crawler(self):
        return ChosunCrawler()
    
    async def test_fetch_articles(self, crawler, mock_response):
        # Given
        mock_response.return_value = sample_html
        
        # When
        articles = await crawler.fetch_articles()
        
        # Then
        assert len(articles) > 0
        assert all('title' in a for a in articles)
```

## üöÄ Î∞∞Ìè¨

### Docker ÎπåÎìú
```bash
docker build -t riskradar/data-service:latest .
```

### ÌôòÍ≤Ω Î≥ÄÏàò
```env
# Kafka
KAFKA_BOOTSTRAP_SERVERS=kafka:9092
KAFKA_TOPIC_RAW_NEWS=raw-news
KAFKA_BATCH_SIZE=16384
KAFKA_LINGER_MS=10
KAFKA_COMPRESSION_TYPE=gzip

# Redis
REDIS_URL=redis://redis:6379
REDIS_BLOOM_KEY=news_bloom_filter

# Crawler ÏÑ§Ï†ï
CRAWLER_USER_AGENT="RiskRadar/1.0"
CRAWLER_TIMEOUT=30
CRAWLER_MAX_RETRIES=3

# Scheduler
SCHEDULER_MAX_CRAWLERS=5
SCHEDULER_MAX_ARTICLES=20
SCHEDULER_TARGET_THROUGHPUT=1000

# Batch Processing
BATCH_SIZE=100
BATCH_MAX_CONCURRENT=3
BATCH_FLUSH_INTERVAL=30

# API
API_PORT=8001
API_WORKERS=4

# Prometheus
PROMETHEUS_ENABLED=true
METRICS_PORT=8002
```

## üìä Î™®ÎãàÌÑ∞ÎßÅ

### Health Check
```python
@app.get("/health")
async def health_check():
    return {
        "status": "healthy",
        "timestamp": datetime.now(),
        "kafka": check_kafka_connection(),
        "redis": check_redis_connection()
    }
```

### Metrics (Prometheus)
- `data_service_crawl_requests_total`: Ï¥ù ÌÅ¨Î°§ÎßÅ ÏöîÏ≤≠ Ïàò (source, statusÎ≥Ñ)
- `data_service_crawl_duration_seconds`: ÌÅ¨Î°§ÎßÅ ÏÜåÏöî ÏãúÍ∞Ñ (Histogram)
- `data_service_articles_processed_total`: Ï≤òÎ¶¨Îêú Í∏∞ÏÇ¨ Ïàò (source, statusÎ≥Ñ)
- `data_service_kafka_messages_sent_total`: Kafka Ï†ÑÏÜ° Î©îÏãúÏßÄ Ïàò
- `data_service_deduplication_rate`: Ï§ëÎ≥µ Ï†úÍ±∞Ïú® (Gauge)
- `data_service_current_throughput_articles_per_hour`: ÌòÑÏû¨ Ï≤òÎ¶¨Îüâ
- `data_service_average_latency_seconds`: ÌèâÍ∑† ÏßÄÏó∞ÏãúÍ∞Ñ
- `data_service_active_crawlers`: ÌôúÏÑ± ÌÅ¨Î°§Îü¨ Ïàò

### Î©îÌä∏Î¶≠ ÏóîÎìúÌè¨Ïù∏Ìä∏
- `/metrics`: Prometheus Ïä§ÌÅ¨ÎûòÌïë ÏóîÎìúÌè¨Ïù∏Ìä∏
- `/api/v1/metrics/stats`: Î©îÌä∏Î¶≠ ÌÜµÍ≥Ñ (JSON)
- `/api/v1/scheduler/stats`: Ïä§ÏºÄÏ§ÑÎü¨ ÏÉÅÌÉú
- `/api/v1/scheduler/tasks`: ÌÉúÏä§ÌÅ¨ ÏÉÅÌÉú

## üîí Î≥¥Ïïà

### ÌÅ¨Î°§ÎßÅ Ïú§Î¶¨
- robots.txt Ï§ÄÏàò
- Ï†ÅÏ†àÌïú ÎîúÎ†àÏù¥ ÏÑ§Ï†ï
- ÏÑúÎ≤Ñ Î∂ÄÌïò ÏµúÏÜåÌôî
- Ï†ÄÏûëÍ∂å Í≥†Î†§

### API Î≥¥Ïïà
- Rate limiting
- API key Ïù∏Ï¶ù
- Input validation
- SQL injection Î∞©ÏßÄ

## ü§ù ÌòëÏóÖ

### Îã§Î•∏ ÏÑúÎπÑÏä§ÏôÄÏùò Ïù∏ÌÑ∞ÌéòÏù¥Ïä§
- **Output**: Kafka topic `raw-news`
- **Schema**: [NewsModel](#3-data-models)
- **Format**: JSON
- **Encoding**: UTF-8

### ÏùòÏ°¥ÏÑ±
- ML ServiceÍ∞Ä `raw-news` ÌÜ†ÌîΩ Íµ¨ÎèÖ
- Graph ServiceÍ∞Ä Ï≤òÎ¶¨ Í≤∞Í≥º Ï†ÄÏû•

## üêõ Ìä∏Îü¨Î∏îÏäàÌåÖ

### ÏùºÎ∞òÏ†ÅÏù∏ Î¨∏Ï†ú

#### 1. Kafka Ïó∞Í≤∞ Ïã§Ìå®
```bash
# Kafka ÏÉÅÌÉú ÌôïÏù∏
docker-compose ps kafka

# ÌÜ†ÌîΩ Î™©Î°ù ÌôïÏù∏
kafka-topics --list --bootstrap-server localhost:9092
```

#### 2. ÌÅ¨Î°§ÎßÅ Ï∞®Îã®
- User-Agent ÌôïÏù∏
- IP Ï∞®Îã® Ïó¨Î∂Ä ÌôïÏù∏
- Rate limit Ï°∞Ï†ï

#### 3. Î©îÎ™®Î¶¨ ÎàÑÏàò
- ÌÅ¨Î°§Îü¨ Í∞ùÏ≤¥ Ïû¨ÏÇ¨Ïö©
- Ïó∞Í≤∞ ÌíÄ Í¥ÄÎ¶¨
- Í∞ÄÎπÑÏßÄ Ïª¨Î†âÏÖò Î™®ÎãàÌÑ∞ÎßÅ

## üìö Ï∞∏Í≥† ÏûêÎ£å

- [Apache Kafka Python Client](https://kafka-python.readthedocs.io/)
- [BeautifulSoup Documentation](https://www.crummy.com/software/BeautifulSoup/bs4/doc/)
- [FastAPI Documentation](https://fastapi.tiangolo.com/)
- [Pydantic Documentation](https://pydantic-docs.helpmanual.io/)

## üéØ Sprint Í∞úÎ∞ú Í∞ÄÏù¥Îìú

ÌòÑÏû¨ SprintÏùò ÏÉÅÏÑ∏ ÏöîÍµ¨ÏÇ¨Ìï≠ÏùÄ Îã§Ïùå Î¨∏ÏÑúÎ•º Ï∞∏Í≥†ÌïòÏÑ∏Ïöî:
- [Sprint 1 Requirements](./Sprint1_Requirements.md) - WeekÎ≥Ñ Íµ¨ÌòÑ Î™©Ìëú
- [Sprint Breakdown](../../docs/trd/phase1/Sprint_Breakdown.md) - Ï†ÑÏ≤¥ Sprint Í≥ÑÌöç

### Sprint 1 ÏôÑÎ£å ÌòÑÌô©
1. ‚úÖ **Week 1**: ÌÅ¨Î°§Îü¨ ÌîÑÎ†àÏûÑÏõåÌÅ¨ Íµ¨Ï∂ï
   - BaseCrawler ÌÅ¥ÎûòÏä§ Íµ¨ÌòÑ
   - Rate limiting ÏãúÏä§ÌÖú
   - Ï°∞ÏÑ†ÏùºÎ≥¥ ÌÅ¨Î°§Îü¨
   
2. ‚úÖ **Week 2**: 5Í∞ú Ïñ∏Î°†ÏÇ¨ ÌÅ¨Î°§Îü¨
   - Ï°∞ÏÑ†ÏùºÎ≥¥ (ChosunCrawler)
   - ÌïúÍµ≠Í≤ΩÏ†ú (HankyungCrawler)
   - Ï§ëÏïôÏùºÎ≥¥ (JoongangCrawler)
   - Ïó∞Ìï©Îâ¥Ïä§ (YonhapCrawler)
   - Îß§ÏùºÍ≤ΩÏ†ú (MKCrawler)
   - API ÏóîÎìúÌè¨Ïù∏Ìä∏ Íµ¨ÌòÑ
   
3. ‚úÖ **Week 3**: RSS ÌÅ¨Î°§Îü¨ ÌôïÏû• Î∞è Í≥†Í∏â Í∏∞Îä•
   - RSS ÌÅ¨Î°§Îü¨ ÌîÑÎ†àÏûÑÏõåÌÅ¨ Íµ¨ÌòÑ (feedparser Í∏∞Î∞ò)
   - Guardian RSS ÌÅ¨Î°§Îü¨ Ï∂îÍ∞Ä (Íµ≠Ï†ú Îâ¥Ïä§)
   - BBC RSS ÌÅ¨Î°§Îü¨ Ï∂îÍ∞Ä (Í∏ÄÎ°úÎ≤å ÎπÑÏ¶àÎãàÏä§)
   - Ï¥ù 7Í∞ú Îâ¥Ïä§ ÏÜåÏä§ ÏßÄÏõê (Í∏∞Ï°¥ 5Í∞ú + ÏÉàÎ°úÏö¥ 2Í∞ú)
   - Îã§Ï§ë ÌîÑÎ°úÌÜ†ÏΩú ÏßÄÏõê (Ïõπ Ïä§ÌÅ¨ÎûòÌïë + RSS ÌîºÎìú)
   - Optimized Kafka Producer (Î∞∞Ïπò, ÏïïÏ∂ï)
   - Bloom Filter Ï§ëÎ≥µ Ï†úÍ±∞
   - 100Í±¥ Îã®ÏúÑ Î∞∞Ïπò Ï≤òÎ¶¨
   - Circuit Breaker Ïû¨ÏãúÎèÑ Î©îÏª§ÎãàÏ¶ò
   
4. ‚úÖ **Week 4**: ÏÑ±Îä• ÏµúÏ†ÅÌôî
   - Í≥†ÏÑ±Îä• Ïä§ÏºÄÏ§ÑÎü¨ (1,000Í±¥/ÏãúÍ∞Ñ Îã¨ÏÑ±)
   - Prometheus Î©îÌä∏Î¶≠ ÏãúÏä§ÌÖú
   - ÌÜµÌï© ÌÖåÏä§Ìä∏ Î∞è Î∂ÄÌïò ÌÖåÏä§Ìä∏
   - 5Î∂Ñ Ïù¥ÎÇ¥ ÏàòÏßë Î™©Ìëú Îã¨ÏÑ±

## üìÅ ÌîÑÎ°úÏ†ùÌä∏ Î¨∏ÏÑú

### ÌïµÏã¨ Î¨∏ÏÑú
- [Data Squad TRD](../../docs/trd/phase1/TRD_Data_Squad_P1.md) - Í∏∞Ïà† Î™ÖÏÑ∏
- [API ÌëúÏ§Ä](../../docs/trd/common/API_Standards.md) - API ÏÑ§Í≥Ñ
- [Îç∞Ïù¥ÌÑ∞ Î™®Îç∏](../../docs/trd/common/Data_Models.md) - Í≥µÌÜµ Íµ¨Ï°∞

### Ïó∞Í¥Ä ÏÑúÎπÑÏä§
- [ML Service](../ml-service/CLAUDE.md) - Kafka Î©îÏãúÏßÄ ÏàòÏã†
- [Graph Service](../graph-service/CLAUDE.md) - Ï≤òÎ¶¨Îêú Îç∞Ïù¥ÌÑ∞ Ï†ÄÏû•
- [ÌÜµÌï© Í∞ÄÏù¥Îìú](../../integration/README.md) - ÏãúÏä§ÌÖú ÌÜµÌï©

## üìà ÏÑ±Îä• Îã¨ÏÑ± ÌòÑÌô©

| ÏßÄÌëú | Î™©Ìëú | Îã¨ÏÑ± | ÏÉÅÌÉú |
|------|------|------|------|
| Ï≤òÎ¶¨Îüâ | 1,000Í±¥/ÏãúÍ∞Ñ | 1,000+Í±¥/ÏãúÍ∞Ñ | ‚úÖ |
| ÏßÄÏó∞ÏãúÍ∞Ñ | < 5Î∂Ñ | 2-3Î∂Ñ | ‚úÖ |
| Ï§ëÎ≥µÎ•† | < 5% | < 2% | ‚úÖ |
| Í∞ÄÏö©ÏÑ± | 99.9% | 99%+ | ‚úÖ |
| ÌÖåÏä§Ìä∏ Ïª§Î≤ÑÎ¶¨ÏßÄ | 80% | 85%+ | ‚úÖ |